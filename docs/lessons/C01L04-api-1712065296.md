Przechodzimy już do bezpośredniego połączenia z modelem poprzez OpenAI API oraz wprowadzeniem do **koncepcji** frameworka LangChain (JavaScript). Ze względu na to, że możesz programować w innych technologiach lub posługiwać się narzędziami no-code, skupimy się na praktykach, które możesz połączyć ze swoimi umiejętnościami.

## Ustawienia konta OpenAI

API pozwala nam nawiązywać bezpośredni kontakt z modelami GPT. W przeciwieństwie do ChatGPT Plus tutaj model rozliczeń opiera się o **liczbę przetworzonych tokenów**. Przez długi czas rozliczenie za zużycie tokenów odbywało się na zakończenie miesiąca rozliczeniowego. Z tego powodu krytycznie ważne było ustawienie limitów wydatków, aby nie skończyć z rachunkiem na $120 (był to domyślny limit na każdym z nowych kont).

Obecnie sytuacja się zmieniła (i to całkiem niedawno, bo od 18 marca, czyli od dnia startu szkolenia) i model rozliczeń z OpenAI zmienił się na płatność z góry. Czyli obecnie konto trzeba doładować zawczasu odpowiednią kwotą. Mamy wtedy pewność, że nie wydamy ani grosza ponad kwote, którą doładowaliśmy konto.

Zalecamy doładowanie konta kwotą **$10**, ale praktyka z poprzednich edycji pokazała, że kursanci w większości przypadków mieścili się w opłatach nawet połowę niższych. Dawniej minimalną kwotą doładowania było **$1**, a obecnie jest to **$5**.

Dla własnej wygody, aby nagle nie zostać odciętym od API, sugerujemy zastosowanie wspomnianej wcześniej kwoty. Zawsze można ją później wykorzystać po szkoleniu na własne potrzeby.

[https://platform.openai.com/account/billing/overview](https://platform.openai.com/account/billing/overview)

![](https://assets.circle.so/ayq2sv5lxux4p818jbqf63ofh5ts)

Jeśli dopiero zakładasz konto, nie będziesz mieć dostępu do GPT-4 a jedynie do modeli GPT-3.5-Turbo czy GPT-3.5-Turbo-16k. Dostęp pojawi się po wykonaniu pierwszej płatności.

**UWAGA**: jeśli pomimo doładowania konta nie masz dostępu do GPT-4, to wyloguj się i zaloguj ponownie. Przeważnie to rozwiązuje problem.

![](https://assets.circle.so/ty65ukcz87sdiocic5bk38pgqz74)

  

Gdy Twoje konto zostanie doładowane odpowiednią kwotą, pobierz klucz API z zakładki [API keys](https://platform.openai.com/account/api-keys) i możemy przejść dalej.

## Nawiązanie połączenia z OpenAI

[API.mp4](https://assets.circle.so/4mtw44aavmln7h7jy0b990wb48h8)

Zakładam, że z pomocą znanych Ci narzędzi jesteś w stanie wykonać proste połączenie HTTP autoryzowane kluczem API. Warto jednak rozważyć **skorzystanie z SDK**, a w przypadku platform no-code, np. [make.com](http://make.com/) z **natywnych modułów**.

![](https://assets.circle.so/wyv518r2etbz3g4h6yas2vzzkb4n)

  
Minimalny kontakt z modelem wygląda tak, jak poniżej. Zatem mówimy o zapytaniu typu POST z właściwościami **message** oraz **model**. Jako odpowiedź otrzymujemy rozbudowany obiekt, w przypadku którego najbardziej interesuje nas lista **choices** zawierająca odpowiedź modelu. Istotna może być także właściwość **‌usage** zawierająca informacje o wykorzystanych tokenach, które możemy zapisać po swojej stronie.

![](https://assets.circle.so/saz35gel9rfe68q6v8cnrjtsnk31)

Najprostszym sposobem wykonania powyższego polecenia, jest wykorzystanie systemowego terminala / linii poleceń i zapytania CURL (oczywiście należy ustawić swój klucz API):

```
curl -X POST \
   https://api.openai.com/v1/chat/completions \
   -H "Authorization: Bearer OPENAI_API_KEY" \
   -H 'Content-Type: application/json' \
   -d '{
     "messages": [{ "role": "user", "content": "Hello!" }],
     "model": "gpt-3.5-turbo"
   }'
```

API, poza parametrami, które widzieliśmy przy okazji Playground, oferuje kilka dodatkowych ustawień. Dwa z nich są bezpośrednio związane z **Function Calling**, którym nie będziemy się teraz zajmować i wrócimy do nich przy okazji jednej z kolejnych lekcji. Pozostałe prezentują się następująco:

*   **n** — w związku z niedeterministyczną naturą modeli, możliwe jest **wygenerowanie kilku odpowiedzi dla tego samego promptu**. Jest to przydatne na potrzeby zastosowania "[self-consistency](https://arxiv.org/abs/2307.06857)" polegającym na sklasyfikowaniu i wyborze najlepszej odpowiedzi. Trzeba jednak pamiętać, że generowanie wariantów **wpływa na zużycie tokenów i tym samym większe koszty.**
    
*   **stream** — generowanie odpowiedzi zajmuje czas, dlatego OpenAI oferuje opcję strumieniowania, umożliwiając nam odczytywanie pojedynczych tokenów, co nierzadko ma **szczególnie pozytywny wpływ na doświadczenia użytkowników**
    
*   **logit\_bias** — osobiście nie miałem jeszcze okazji wykorzystywać tej opcji. Jej zadanie polega **obniżeniu prawdopodobieństwa wyboru wskazanych tokenów**. np. {11088: -100} zniechęca model do wykorzystywania słowa "kill". Wartość tokenu można uzyskać z pomocą encodera, który omówimy za moment
    
*   **user** — w przypadku zastosowania produkcyjnego, [dobre praktyki bezpieczeństwa](https://platform.openai.com/docs/guides/safety-best-practices) uwzględniają KYC (know your customer). Powodem jest fakt, że jeśli przykładowo udostępniasz na stronie chatbota, a jeden z użytkowników zapyta go o rzeczy, które naruszają politykę OpenAI, **Twoje całe konto może zostać zablokowane**. Korzystając z ID użytkownika, możesz wykorzystać tę informację na potrzeby takich sytuacji.
    

W ostatniej części kursu będziemy korzystać (opcjonalnie) z platformy [make.com](http://make.com/), która znacząco ułatwia tworzenie różnych integracji, a nawet narzędzi, którymi model może się posługiwać. Aby nawiązać w niej połączenie z OpenAI, możesz skorzystać z wbudowanego modułu HTTP lub (rekomendowane) z natywnego modułu OpenAI. Wystarczy, że utworzysz nowy scenariusz i z listy dostępnych integracji wybierzesz OpenAI, a następnie zapiszesz swój klucz API.

![](https://assets.circle.so/uzginvd3rqerbko5amoxepc5owi2)

Na ten moment, to wszystko.

## LangChain

Połączenie LLM z logiką aplikacji lub scenariusza automatyzacji wydaje się prosta. W praktyce jednak, mamy tutaj do czynienia z połączeniem **naturalnego języka** i niedeterministycznej natury modeli z **kodem precyzyjnie opisującym przepływ danych**. Sytuacja zaczyna się komplikować w momencie, gdy projektujemy system składający się z większej liczby promptów, które mogą (ale nie muszą) się ze sobą łączyć.

Z tego powodu powstają różne narzędzia, których celem jest **ułatwienie nam tego zadania**. Każde z nich znajduje się jednak obecnie na stosunkowo wczesnym etapie rozwoju. Nie wszystkie ich elementy są dopracowane, a część z nich bardziej przeszkadza, niż pomaga. Przykładami rozwiązań, na które warto zwrócić uwagę są — LangChain ([Python](https://langchain.readthedocs.io/en/latest/) / [JavaScript](https://js.langchain.com/docs/)), na którym skupi się nasza uwaga, oraz [LLaMA Index](https://www.llamaindex.ai/).

LangChain obecnie jest bardzo rozbudowany, jednak w wielu miejscach **narzuca zbyt dużą warstwę abstrakcji** i/lub **posiada nieprecyzyjną dokumentację**. Jednocześnie przedstawia różne koncepcje, które możemy implementować samodzielnie lub korzystać tylko z wybranych narzędzi, całkowicie ignorując pozostałe.

### **Połączenie z modelem**

LangChain oferuje rozbudowany interfejs dla różnych modeli (OpenAI/PaLM/Anthropic/Ollama), **co ułatwia ich ewentualne łączenie.** Sama interakcja do złudzenia przypomina tą, znaną z SDK. Mamy tu zatem zainicjowanie. połączenia z modelem oraz faktyczne wysłanie zapytania, w tym przypadku w formacie ChatML (podział system/user/assistant). Poniżej przykład [01\_langchain\_init](https://github.com/i-am-alice/2nd-devs/tree/main/01_langchain_init).

![](https://assets.circle.so/rre4tcrhhuefkvzl708gk49vsdu0)

  
Istotna różnica pojawia się na etapie **integrowania promptów** oraz **odpowiedzi modelu** z kodem aplikacji, np. poprzez możliwość **zweryfikowania formatu odpowiedzi**, **szablony promptów** i **ich kompozycję**. Pomimo tego, że w przypadku niektórych języków, np. JavaScript z powodzeniem możemy skorzystać np. [Tag Function / Tagged Templates](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#tagged_templates), to warto rozważyć strukturyzowanie promptów z pomocą wbudowanych w LangChain metod.

Poniżej odwzorowałem jeden z promptów, który omawialiśmy w poprzednich lekcjach, wykorzystując Prompt Templates. Dokładnie **taki sam efekt** mógłbym osiągnąć z pomocą Tag Function lub nawet prostym łączeniem ciągów znaków. Jednak tutaj różnica pojawia się w chwili, gdy kod naszej aplikacji staje się bardziej złożony. W związku z podatnością promptów na nawet najmniejsze zmiany, stosowanie mechanizmów umożliwiających **utrzymanie struktury** jest pomocne ([02\_langchain\_format](https://github.com/i-am-alice/2nd-devs/tree/main/02_langchain_format))

![](https://assets.circle.so/lqtrwhmri3gsnyjnzzb06675ilhk)

Podobne mechaniki można stosować także bezpośrednio w automatyzacjach [make.com](http://make.com/) poprzez obecne w nim funkcje i zmienne. Poniżej znajduje się jedno z narzędzi, którymi posługuje się model w celu zarządzania moją listą zadań. Jeśli nie korzystasz z takich narzędzi, to powiem tylko, że sam nie wyobrażam sobie budowania aplikacji łączących się z kilkoma usługami (nierzadko przez OAuth2.0) tylko na swoje potrzeby. Zwyczajnie nie starczyłoby mi na to wszystko czasu, dlatego właśnie wspominam o [make.com](http://make.com/), który świetnie sprawdza się jako uzupełnienie (i nierzadko jako fundament) interakcji z LLM.

![](https://assets.circle.so/t8flku7dhxzd1wwp78kpt3jdx2n3)

Myślę, że widzisz tutaj wyraźnie zmienne, a nawet całe sekcje promptów, które **dynamicznie pojawiają się w jego treści**. To właśnie w tym miejscu znaczenia nabiera **formatowanie promptu** czy **wyraźne wyróżnianie jego poszczególnych sekcji**.

### **Streaming**

W przeciwieństwie do ChatGPT bezpośrednia integracja z OpenAI API przerzuca na nas ciężar obsługi **strumieniowania** ([03\_langchain\_stream](https://github.com/i-am-alice/2nd-devs/tree/main/03_langchain_stream)) i **kontroli limitu tokenów**. Jeszcze kilka miesięcy temu, strumieniowanie wymagało ręcznej implementacji. Dziś mamy do dyspozycji funkcje uruchamiane [podczas generowania tokenów](https://js.langchain.com/docs/modules/model_io/chat/streaming) czy [w chwili wywołania zdarzenia](https://js.langchain.com/docs/modules/model_io/chat/subscribing_events) (np. wystąpienia błędu).

![](https://assets.circle.so/8foxvehomwuprt2c2esw1fbyzg2x)

Jednym z ciekawszych zastosowań strumieniowania z jakim miałem styczność, było połączenie usługi ElevenLabs generującej głos na podstawie tekstu z GPT-4. Normalnie takie zadanie wymagałoby wygenerowania **pełnej odpowiedzi GPT-4**, następnie **pełnego audio ElevenLabs** i ostatecznie **odtworzenia go**. Strumieniowanie sprawia, że z kilkunastu sekund, czas reakcji spadł do ~1.75s

![](https://assets.circle.so/4wf86ztstbmc6rl0b3zy6udid59r)

Wskazówka: "na produkcji" prawdopodobnie będziesz streamować odpowiedzi modelu do klienta, np. przeglądarki. Jeśli będziesz mieć potrzebę przekazania dodatkowych informacji wraz z zapytaniem (np. identyfikatora konwersacji), skorzystaj z **nagłówków HTTP zapytania, np. x-conversation-id.**

**Kontrola Token Window**

Poza strumieniowaniem, połączenie przez API wymaga od nas przejęcia kontroli nad **limitami tokenów**, który łatwo przekroczyć np. podczas rozmowy. Istotnym elementem pracy z dopuszczalnym limitem jest **liczenie tokenów** z pomocą **tiktoken'a**. Jednak [zgodnie z OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb), ze względu na aktualizację modeli, mówimy tutaj o **przybliżonej estymacji**.

Dodatkowo, dla modeli GPT-3.5-Turbo oraz GPT-4, musimy uwzględnić także fakt, że mamy do czynienia z ChatML. Przykładowo, pomimo tego, że fraza "Hey, you!" to 4 tokeny, estymowany rezultat wynosi 11, ponieważ bierzemy pod uwagę metadane wynikające ze struktury ChatML. Zwróć także uwagę na sposób pobierania **token ID**, który możesz wykorzystać w połączeniu z parametrem **logit\_bias**. ([04\_tiktoken](https://github.com/i-am-alice/2nd-devs/tree/main/04_tiktoken))

![](https://assets.circle.so/8gjesa7a7m4xq9v1r14q1t04d3yr)

Uzyskany wynik, w tym przypadku, zgadza się z informacją z właściwości **usage** dla wykonanego zapytania. Wygląda więc na to, że nasz licznik działa.

![](https://assets.circle.so/2iwfoznbqfzp0vrsm2cycskpnwhe)

  
Estymacja tokenów przydaje się praktycznie na każdym kroku podczas integracji z LLM, jednak sama informacja o ich liczbie nie jest wystarczająca i musimy podjąć faktyczne działania związane z kontrolą liczby tokenów w prompcie. Opcji jest kilka:

*   Skorzystanie z modelu wspierającego większą liczbę tokenów, np. GPT-3.5-Turbo-16k (o ile to możliwe)
    
*   Wybór różnych wersji promptu lub jego fragmentów
    
*   Zmniejszenie kontekstu lub ucięcie wcześniejszych wiadomości konwersacji
    
*   Zastosowanie kompresji (w postaci np. podsumowania) dla informacji zawartych w kontekście / konwersacji
    

Zasadniczo dążymy tutaj do utrzymania promptu w ramach "token window", **mając także na uwadze pozostawienie przestrzeni na odpowiedź modelu**. Poniższy obrazek obrazuje tzw. "floating window", czyli przesuwające się wraz z rozmową okno, które ucina wcześniejsze fragmenty rozmowy.

![](https://assets.circle.so/ffpe3pmasam7bji6450ywkmxvfy8)

LangChain daje nam do dyspozycji gotowe mechanizmy wykorzystujące tzw. **ConversationChain** (jest to łańcuch akcji ułatwiających prowadzenie konwersacji) do którego możemy podłączyć **pamięć** krótkoterminową lub długoterminową. Można to porównać do **stanu aplikacji** przechowywanego w pamięci lub pobieranego z bazy danych. O jej konkretnych rodzajach będziemy jeszcze mówić. Tymczasem, spójrzmy na przykład poniżej.

**BufferWindowMemory** jest dokładnie wspomnianym przed sekundą "pływającym oknem". Ustawienie jego parametru "k" na wartość 1 sprawia, że model pamięta **wyłącznie poprzednią interakcję**. Oznacza to, że gdy się przedstawię, poproszę, aby poczekał, a następnie zapytam o moje imię, **to przy założeniu, że w drugiej interakcji go nie powtórzył, nie będzie w stanie go podać** ([05\_conversation](https://github.com/i-am-alice/2nd-devs/blob/main/05_conversation/05.ts))

![](https://assets.circle.so/2o6og0p5x56lnjxc9z4np89klyb4)

Ucząc się na temat zarządzania kontekstem, **możesz pomyśleć, że nie ma to większego sensu**, ponieważ już teraz [istnieją modele zdolne do przetwarzania 1 miliona tokenów](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/) w jednym zapytaniu.

Jednak długi kontekst nadal przekłada się na większe koszty, wydłużenie czasu odpowiedzi, oraz na potencjalne ryzyko **odwrócenia uwagi modelu** od tego, co jest istotne. W rezultacie nawet większe okno kontekstu nie adresuje problemów, które będziemy mogli rozwiązać z pomocą kontekstu budowanego dynamicznie.

Choć model Gemini 1.5 Pro nie jest jeszcze publicznie dostępny, to już teraz do dyspozycji mamy Claude 3, w przypadku którego limit okna kontekstu wynosi aż 200 000.

![](https://assets.circle.so/zfa3q54hlzbrstgpa6c5fgb6o6qn)

Zatem **kontrolowanie limitu tokenów to szukanie balansu** pomiędzy dostarczeniem **istotnych dla bieżącej konwersacji informacji** oraz ich ilością.

Na ten moment, to wszystko na temat LangChain, choć do jego dalszych możliwości wrócimy w kolejnych lekcjach.

## Moderowanie wejścia i wyjścia

Wspomniałem już o KYC oraz [dobrych praktykach od OpenAI](https://platform.openai.com/docs/guides/safety-best-practices) związanych z bezpieczeństwem. Jest to jednak na tyle istotny temat, że musimy zatrzymać się przy nim nieco dłużej.

Istnieje kilka problemów związanych z **dawaniem użytkownikom dowolności pod kątem wprowadzania danych do modelu** oraz **dostarczaniem odpowiedzi generowanych przez model bezpośrednio do użytkowników**. Są to wyzwania związane z bezpieczeństwem aplikacji, bezpieczeństwem samych użytkowników, wyzwaniami prawnymi, zagrożeniami związanymi z wizerunkiem oraz z samą stabilnością aplikacji.

Pierwszym elementem, który warto stosować, jest **Moderation API** od OpenAI, czyli specjalny adres na który możemy przesłać **dowolny tekst**, aby uzyskać informację o tym, **czy narusza politykę OpenAI**. Dla odmiany, poniższe zapytanie zrealizowałem w [make.com](http://make.com/), wprowadzając tekst "You're id\*\*t!", który został **oflagowany**, a więc nie powinien być przekazany do OpenAI.

Sugestia: Moderację OpenAI stosuj także w przypadku swoich własnych danych. Wśród przetwarzanych przez Ciebie treści mogą przypadkowo znajdować się fragmenty niezgodne z polityką OpenAI. Prostym przykładem jest **transkrypcja**, ponieważ tam mogą wystąpić błędy takie jak odczytanie słowa "focus" jako "f\*\*k you" — doświadczyłem tego osobiście.

![](https://assets.circle.so/jdq7osnhisinzb8rbeloav6o9wrr)

Drugim elementem, jest **walidacja danych pod kątem Twoich zasad,** o czym mówi [Constitutional AI](https://arxiv.org/abs/2212.08073). Chodzi tutaj o **dodatkowy prompt**, którego zadaniem jest **sklasyfikowanie dostarczonych danych**. Jeżeli dane wejściowe lub wygenerowana odpowiedź złamie przewidziane przez Ciebie zasady, **zostanie odrzucona** i najlepiej także **oznaczona.** Niestety nie mogę zaprezentować Ci promptów, które sam stosuję, natomiast ich ogólna struktura uwzględnia:

*   Klasyfikację pod kątem "adversarial prompt / prompt injection"
    
*   Wykrycie próby nadpisania zachowania asystenta
    
*   Wykrycie próby przechwycenia instrukcji systemowej
    
*   Wykrycie próby oszukania asystenta
    
*   Wykrycie potencjalnego naruszenia jednej ze zdefiniowanych przeze mnie zasad, dotyczących mojego systemu oraz niepożądanych przeze mnie zachowań, które nierzadko są charakterystyczne dla modelu
    

Taki prompt zwraca 0 lub 1 w zależności od tego, czy wykrył jakieś problemy. Naturalnie taki mechanizm można rozbudować lub przebudować w zależności od swoich potrzeb. **Nie daje on gwarancji bezpieczeństwa**, lecz obniża ryzyko problemów. Dokładnie taki sam prompt może weryfikować także **wypowiedź asystenta**. Wówczas oczywiście niemożliwe jest jej strumieniowanie, co obniża szybkość działania aplikacji.

Trzecim elementem jest **sam prompt**, który powinien zawierać **precyzyjne instrukcje** oraz **sterować zachowaniem modelu tak, aby unikał niepożądanych aktywności oraz wypowiedzi**. Oczywiście nadal jego głównym zadaniem będzie zrealizowanie założonego celu, jednak musi się to odbyć w ramach wcześniej zdefiniowanych założeń. Np. nie chcemy, aby system analizujący dokumenty pod kątem nieścisłości, zaczął odpowiadać na pytania zawarte we fragmentach takich plików.

Ostatecznie uzyskanie większej kontroli nad zachowaniem promptu wykorzystuje mechanizmy określane jako **Guardrails**, np. [NeMo](https://github.com/NVIDIA/NeMo-Guardrails). W przypadku LangChain koncepcja ta występuje w ramach łańcuchów (Chains). Oczywiście też nic nie stoi na przeszkodzie, aby samodzielnie projektować "tory po których porusza się LLM" zintegrowany z naszą aplikacją.

## Integracja z OpenAI API

Pracę z OpenAI API oraz eksplorowanie tego, co oferuje, najlepiej jest zacząć od tworzenia rozwiązań **na swoje potrzeby**. Część z nich użytecznością będzie przypominać "aplikacje to-do" realizowane w ramach nauki. Z kolei inne mogą pozostać z Tobą przez długi czas, oszczędzając Ci energię lub ułatwiając pracę czy naukę.

Techniczne doświadczenie otwiera Ci możliwość **zintegrowania usług i urządzeń z API OpenAI**. GPT-3.5/GPT-4 może być dla Ciebie dostępne praktycznie w dowolnym miejscu na Twoim komputerze poprzez **skróty klawiszowe** oraz realizować różne zadania w tle z pomocą harmonogramu Twojej własnej aplikacji lub scenariuszy automatyzacji.

W każdym przypadku, poza korzyściami dla siebie, stwarzasz sobie przestrzeń do bliższego poznawania Dużych Modeli Językowych. W związku z tym, że chcemy pracować już niemal wyłącznie z API, możemy sięgnąć po gotowe narzędzia lub zbudować własne. Mowa tutaj o:

*   Makrach Siri Shortcuts lub Keyboard Maestro (MacOS)
    

*   Szablonie aplikacji Tauri przygotowanym przeze mnie (macOS / Windows / Linux) do której masz dostęp Open Source
    
*   Własne skrypty, automatyzacji, mikroserwisy
    

Główną idea opiera się tutaj o **połączenie AI niewymagające zmiany kontekstu** lub **działania AI niewymagające Twojego zaangażowania**. Projektując takie rozwiązania dla siebie, początkowo nie będą stabilne, ale samodzielnie je naprawisz. Zdobędziesz w ten sposób wiedzę, którą z czasem możesz zastosować, tworząc projekty dla klientów, pracodawcy, czy rozwijając własną firmę.

### **Konfiguracja Siri Shortcuts (tylko macOS)**

Shortcuts to aplikacja działająca w ekosystemie urządzeń Apple (macOS / ipadOS / iOS / watchOS). Poniższe makra działają na każdym z tych urządzeń i umożliwiają Ci kontakt z modelami OpenAI. Proste, ale przydatne akcje (przetłumacz w luźnym stylu / popraw tekst / podaj definicję / wyjaśnij błąd) możesz przypisać do skrótów klawiszowych.

![](https://assets.circle.so/yfxu65up21pp5igi0jc9xjm1yna3)

  
Pobierz dwa makra:

*   ⚡ [GPT-4 API Connector](https://www.icloud.com/shortcuts/b8223541849b4e94bf90a24d6722226e) — zaimportuj, otwórz i wklej klucz API
    
*   ⚡ [GPT-4 Hello](https://www.icloud.com/shortcuts/d0cf09cf855a4903a0ce45a134427db9) — zaimportuj, **zduplikuj**, zaktualizuj prompt, przypisz do skrótu klawiszowego. Domyślnie do makra trafi **zawartość Twojego schowka**, a chwilę później odpowiedź modelu zostanie do niego skopiowana, zatem możesz ją po prostu wkleić.
    

  

### **Szablon aplikacji Tauri (macOS / Windows / Linux)**

Na potrzeby AI\_Devs przygotowałem szablon aplikacji Tauri, który można dostosować do swoich potrzeb, a następnie **wygenerować na systemy MacOS / Windows / Linux**. Wcześniej należy:

*   Przejść przez instrukcję Tauri: [https://tauri.app/v1/guides/getting-started/prerequisites](https://tauri.app/v1/guides/getting-started/prerequisites)
    
*   Ustawić skróty klawiszowe w pliku main.ts (linia 39) oraz powiązane z nimi instrukcje systemowe
    
*   Zbudować aplikację
    
*   Podać swój klucz API
    

![](https://assets.circle.so/ws1u3bgt931g6afn5ez6y65o97h1)

Szablon wykorzystuje framework Tauri (Rust / Svelte), jednak do skorzystania z aplikacji **nie trzeba znać żadnej z wymienionych technologii**.

Aplikacja posiada następujące funkcjonalności:

*   podłączenie własnego klucza API
    
*   połączenie z GPT-3.5/4
    
*   interakcja ze schowkiem systemowym
    
*   wyświetlanie odpowiedzi w czacie z obsługą markdown
    
*   możliwość przypisywania skrótów klawiszowych do akcji, które można ustalić w kodzie, przed zbudowaniem aplikacji
    

Dodatkowe informacje:

*   Domyślnie wykorzystywany jest model gpt-3.5-turbo. Można to jednak zmienić w plikach
    
*   Aby rozwijać aplikację należy uruchomić kod w trybie "dev" (**pnpm tauri dev**), a aby zbudować, wystarczy polecenie **pnpm tauri release**.
    
*   Do skorzystania z aplikacji należy wygenerować nowy klucz API oraz ustawić limity (podczas developmentu będziecie dużo testować)
    
*   Klucz API zapisywany jest jako plain text w pliku aidevs.dat. Jego lokalizacja zależy od systemu operacyjnego. W macOS jest to ~/Library/Application Support/com.aidevs.dev. Warto tutaj dodać jakiś rodzaj szyfrowania, aby nasz klucz nie był przechowywany w ten sposób.
    
*   API obsługuje zarówno streaming odpowiedzi (zwracanie fragmentami), jak i generowanie całości
    
*   Skróty klawiszowe rejestrują się automatycznie, w chwili uruchomienia aplikacji
    

⚡ » [Pobierz szablon aplikacji Tauri](https://cloud.overment.com/aidevs2_dist-1698928391.zip) «

**Własne skrypty / mikroserwisy / automatyzacje**

W kolejnych lekcjach i późniejszej części kursu będziemy projektować i wdrażać różne skrypty, małe aplikacje oraz scenariusze automatyzacji. Jeśli chcesz, już teraz możesz stworzyć swoje proste narzędzia, korzystając z ulubionych technologii oraz tego, czego się nauczyliśmy do tej pory. Wówczas kolejne tygodnie dadzą Ci nowe pomysły i przykłady rozszerzeń, które możesz zastosować w przypadku **własnego asystenta AI**

* * *

## Zadania praktyczne

1.  Zastosuj wiedzę na temat działania modułu do moderacji treści i rozwiąż zadanie o nazwie “**moderation**” z użyciem naszego API do sprawdzania rozwiązań. Zadanie polega na odebraniu tablicy zdań (4 sztuki), a następnie zwróceniu tablicy z informacją, które zdania nie przeszły moderacji. Jeśli moderacji nie przeszło pierwsze i ostatnie zdanie, to odpowiedź powinna brzmieć **\[1,0,0,1\].** Pamiętaj, aby w polu ‘answer’ zwrócić tablicę w JSON, a nie czystego stringa.  
    P.S. wykorzystaj najnowszą wersję modelu do moderacji (**_text-moderation-latest_**)
    
2.  Napisz wpis na bloga (w języku polskim) na temat przyrządzania pizzy Margherity. Zadanie w **API** nazywa się ”**blogger**”. Jako wejście otrzymasz spis 4 rozdziałów, które muszą pojawić się we wpisie (muszą zostać napisane przez LLM). Jako odpowiedź musisz zwrócić tablicę (w formacie JSON) złożoną z 4 pól reprezentujących te cztery napisane rozdziały, np.: **{"answer":\["tekst 1","tekst 2","tekst 3","tekst 4"\]}**