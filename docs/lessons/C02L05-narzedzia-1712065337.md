Generowanie odpowiedzi w ustrukturyzowanej formie, np. obiektu JSON, umożliwia nam połączenie modelu z zewnętrznymi narzędziami. Mowa tutaj zarówno o wykorzystaniu funkcji lokalnych wewnątrz naszej aplikacji, ale także możliwości posługiwania się zewnętrznym API.

Problem w tym, że domyślnie LLM mają tendencję do zwracania odpowiedzi zawierającej dodatkowy komentarz, np. "Proszę, oto obiekt JSON, o który prosisz" lub zamykanie ich w bloku składni markdown. Takie zachowanie znacznie utrudnia dalsze wykorzystanie takiej odpowiedzi w kodzie i domyślnie prowadzi nas do wyrażeń regularnych. Istnieje jednak inna ścieżka.

## JSON Mode

Domyślnie strategią na uzyskanie ustrukturyzowanych odpowiedzi ze strony modelu było zaprojektowanie promptu zawierającego szereg przykładów oczekiwanego zachowania. Konieczne było także wspieranie się wyrażeniami regularnymi lub dodatkowymi promptami, które "naprawiały" błędnie wygenerowane dane. Do gry wchodził także fine-tuning który specjalizował model w generowaniu odpowiedzi w oczekiwany sposób. Sam w dokładnie w ten sposób pracowałem z modelem text-davinci-003.

Kilka miesięcy później OpenAI udostępniło tzw. "Function Calling" o którym powiem za chwilę, a zaraz potem "JSON mode", na którym skupimy się teraz. Jak sama nazwa wskazuje, jest to specjalny tryb, wspierany przez wybrane modele OpenAI (ale i nie tylko), który znacznie zwiększe prawdopodobieństwo (ale nie daje pewności), że odpowiedź modelu będzie **poprawnym obiektem JSON**.

Poniżej mamy przykład zapytania do OpenAI, gdzie do modelu **gpt-3.5-turbo-0125** przekazujemy nie tylko listę wiadomości, ale także dodatkowy parametr określający format oczekiwanej odpowiedzi (może to być **text** lub **json\_object**). Warunkiem koniecznym jest skorzystanie tutaj z wersji modelu, która wspiera JSON mode, oraz dodanie do wiadomości system słowa "JSON", które zwykle powinno pojawić się w naszej instrukcji.

![](https://assets.circle.so/tr25l616cbyx9s3303kmc7aijrwp)

Warto dodać, że nie tylko OpenAI (a konkretnie [tylko wybrane modele](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)) oferuje JSON mode, ponieważ obecnie możemy go spotkać także na platformie Groq oraz Ollama (dzięki której uruchomimy modele lokalnie).

![](https://assets.circle.so/qeot69d5n4yse9rh95szdic6vgoc)

Należy pamiętać, że JSON Mode **nie gwarantuje nam, że otrzymamy obiekt zawierający treści, na których nam zależy**. Oznacza to, że choć wygenerowana odpowiedź będzie poprawnym obiektem JSON, to może zawierać błędne klucze czy ich wartości właściwości. Wyraźnie widać to na przykładzie poniżej, gdzie po prostu nie doprecyzowałem tego, jakiej struktury obiektu potrzebuję, więc model zdecydował o niej samodzielnie.

![](https://assets.circle.so/bostajc8ef7tfijlap2ad9gu4jjv)

Zatem jeśli chcemy zastosować JSON Mode w kodzie, powinniśmy zadbać o:

*   Napisanie promptu zawierającego precyzyjny opis oczekiwanej struktury
    
*   Bardzo pomocne będzie uwzględnienie zestawu przykładów, które zwiększą prawdopodobieństwo uzyskania oczekiwanego obiektu
    
*   Pomimo wszystko i tak powinniśmy zadbać o upewnienie się, czy obiekt jest poprawny, oraz czy zawiera wszystkie wymagane przez nas właściwości. W przypadku błędu parsowania obiektu będziemy mogli przechwycić wyjątek (co oferuje nam praktycznie każdy język programowania). Natomiast poszczególne właściwości możemy weryfikować albo programistycznie, albo z pomocą dodatkowych promptów.
    

W temacie ustrukturyzowanych danych należy też podkreślić rolę tokenizacji. Łatwo zauważyć, że nawet prosty obiekt JSON zawiera więcej tokenów niż format YAML. Dlatego w niektórych sytuacjach może być lepiej używać YAML-a zamiast JSON-a. Szczegóły techniczne dotyczące różnicy między tymi formatami można znaleźć w filmie [**Let's build the GPT Tokenizer**](https://www.youtube.com/watch?v=zduSFxRajkE&t=1240s)**.**

![](https://assets.circle.so/6szfembexlx0bg7bv0bwnwcsws2i)

## Połączenie z narzędziami i Function Calling

Samo generowanie obiektów JSON jest jednym z elementów składających się na możliwość połączenia LLM z narzędziami. Powodem jest fakt, że nierzadko będzie nam zależało nie tylko na tym, aby określić, jakie dane mają trafić do danej funkcji, ale także na tym, aby wskazać to, która funkcja ma zostać uruchomiona.

Proces podejmowania decyzji o tym, które z dostępnych narzędzi ma zostać wykorzystane, może być dość złożony i uwzględniać wieloetapową klasyfikację czy **nawet planowanie przyszłych działań na podstawie zewnętrznych danych wczytanych z pamięci długoterminowej.** Oznacza to, że mówiąc o LLM posługującym się narzędziami, zaczynamy wchodzić w obszar tzw. "Agentów AI" zdolnych do realizowania bardzo złożonych zadań.

![](https://assets.circle.so/48eke43eibdovl3de6tyktwz8x1w)

Zasadniczo logikę odpowiedzialną za wybór narzędzi najlepiej jest zaprojektować samodzielnie, ponieważ zwykle będzie nam zależało na dość dużej elastyczności. Zanim jednak sami będziemy w stanie projektować takie mechaniki, przyjrzymy się temu, jak wygląda tzw. "Function Calling" oferowany przez OpenAI, na który również możemy się zdecydować, jeśli tylko okaże się wystarczający.

W praktyce polega to na tym, że:

*   jako dodatkowy parametr zapytania przekazujemy listę funkcji (czy też narzędzi) w postaci **nazwy, opisu oraz zestaw parametrów**
    
*   poza tym przesyłamy także **listę wiadomości**, podobnie jak w klasycznej interakcji z modelem
    
*   model w odpowiedzi zwraca nam **nazwę funkcji** oraz **listę wartości jej parametrów**
    
*   opcjonalnie (ale jest to zwykle potrzebne) wskazujemy, która funkcja ma być wybrana jako domyślna. **Ważne:** jeśli jej nie wskażemy, a model uzna, że żadna z dostępnych funkcji nie powinna zostać uruchomiona, to w odpowiedzi otrzymamy zwykłą wiadomość modelu.
    

Poniżej mamy przykład Function Calling, a konkretnie prośby użytkownika o zapamiętanie pewnej notatki. Domyślnie model nie byłby w stanie wykonać takiej operacji, jednak teraz ma do dyspozycji narzędzie "quick\_note", które może przechwycić treść jego wiadomości.

![](https://assets.circle.so/1mzo4u2sjxznw4kdwev9hc0m8coe)

Wykonanie takiego zapytania generuje odpowiedź zawierającą obiekt JSON (argumenty) oraz nazwę funkcji, którą model wybrał w związku z konwersacją. Można więc powiedzieć, że model **zdecydował o tym, którą funkcję wybrać oraz o tym, jak ją uruchomić**.

Zwróć uwagę także na **liczbę tokenów**, która uwzględnia nie tylko wiadomości, ale także definicje funkcji. Z technicznego punktu widzenia są one **wstrzykiwane do instrukcji systemowej**. W praktyce oznacza to mniej więcej tyle, że **jesteśmy ograniczeni kontekstem** i że budując interakcje wykorzystujące Function Calling **zwykle będziemy musieli posługiwać się krótką listą, aby** nie przekroczyć dostępnego limitu. Dodatkowo dla samego opisu pojedynczej funkcji czy jej parametrów obowiązuje limit tokenów, ale dokumentacja ani komunikaty błędów nie mówią dokładnie, ile on wynosi.

Na potrzeby Function Calling, parametr **tools** zawierający listę dostępnych funkcji i ich parametrów, można opisać następująco:

![](https://assets.circle.so/psuua8xtnb5gznyfjs2goxoovq31)

*   [Zobacz gist](https://gist.githubusercontent.com/iceener/7faf09a42c3626854987bb9eab162dad/raw/f73f49841e67205b6bb2194bcd54c3d2d813aebf/function_calling.json)
    

Zatem dostępne typy danych to **number, integer, string, object, array, boolean, null**. Dodatkowo przy obiektach i tablicach musimy określić jeszcze typ elementów. Możliwe jest także zdefiniowanie właściwości **enum** pozwalającej na ograniczenie wartości to określonej listy. Daj sobie chwilę czasu na przeanalizowanie powyższego obiektu. Niestety w dniu pisania tego tekstu, dokumentacja OpenAI nie opisuje dokładnie powyższej struktury i dochodziłem do niej metodą prób i błędów. Możliwe więc, że są jeszcze nieudokumentowane właściwości, które pominąłem.

Przy pracy z **Function Calling** należy pamiętać, że nie mamy tutaj do czynienia z jakimś magicznym mechanizmem, a jedynie pewną warstwą abstrakcji oraz dostosowanym modelem. Z tego powodu **nadal obowiązują tutaj wyzwania związane z halucynacją**, **utrzymywaniem uwagi modelu** czy **błędy logiczne**. Cały czas możesz myśleć o tym, jak o prompcie (bo tym jest), lecz w nieco innej strukturze niż zazwyczaj.

Jeśli korzystasz z [make.com](http://make.com/) do interakcji z modelami, to możesz wykorzystać akcję "Transform Text to Structured Data". Nie jest ona tak elastyczna, jak w przypadku bezpośredniego połączenia z API (które w [make.com](http://make.com/) także możesz wykonać), ale jednocześnie jest dość wygodna w użyciu. Po prostu w ramach odpowiedzi otrzymujesz **sformatowany tekst.**

![](https://assets.circle.so/l263mueh6ep15i64o36iullwr44z)

  
Zobaczmy teraz, jak możemy przenieść nasz przykład na kod.

## Function Calling w kodzie aplikacji oraz automatyzacji

Przykład [**13\_functions**](https://github.com/i-am-alice/2nd-devs/blob/main/13_functions/13.ts) przedstawia ogólny schemat pracy z Function Calling w LangChain, aczkolwiek w bardzo podobny sposób wyglądałoby to w przypadku bezpośredniej interakcji z OpenAI API. Zasadniczo musimy zadbać o to, aby upewnić się, że odpowiedź faktycznie zawiera informację o funkcji, którą mamy wykonać, oraz że jej argumenty są zgodne z tym, czego oczekujemy.

![](https://assets.circle.so/cc5rnu7fr16t69ntvar9o9qdjlsv)

Kolejnym krokiem jest **faktyczne wykonanie wskazanej akcji**, co również można zrealizować na różne sposoby, w zależności od tego, czy chcemy wywoływać funkcje **lokalne** (zapisane w kodzie) czy **zdalne** (poprzez zapytania HTTP).

W przypadku tych pierwszych potrzebujemy rodzaju interfejsu (np. obiekt), który pozwoli nam wskazać funkcję, którą chcemy wykonać. Przykład implementacji znajduje się poniżej. Obiekt **functions** zawiera listę dostępnych akcji. Gdy odpowiedź zwrócona przez model wskazuje na jedną z nich, możemy ją uruchomić, przekazując wygenerowane argumenty.

![](https://assets.circle.so/lwd6hdt480319s0p1l39ouzutte9)

  
Dla zapytań HTTP możemy zastosować podobne rozwiązanie, aczkolwiek sam zwykle korzystam z **akcji zdefiniowanych w bazie danych (np. postgreSQL lub Airtable)**. Wówczas sytuacja jest jeszcze prostsza, ponieważ **wskazana nazwa służy do znalezienia wpisu w bazie danych**, a argumenty przekazywane są bezpośrednio jako payload zapytania HTTP. Obrazuje to poniższy przykład, który pochodzi bezpośrednio z kodu źródłowego mojej wersji Alice.

![](https://assets.circle.so/lw4ptqeadab2f1yko95ik89mlxd8)

Ostatecznie do dyspozycji mamy jeszcze platformę [make.com](http://make.com/), w przypadku której opcja strukturyzowania danych może nam się przydać na potrzeby **narzędzi**, którymi będzie posługiwać się np. GPT-4.

![](https://assets.circle.so/4whhrn56xi2d1tlmtnle1sygukhk)

*   [⚡ Pobierz Blueprint Scenariusza](https://cloud.overment.com/aidevs_structured-1695368295.json)
    

## Przykłady narzędzi z których może korzystać GPT-4

Wybór funkcji oraz przygotowanie danych do jej wywołania to koncepcja zmieniająca oblicze praktycznego zastosowania dużych modeli językowych. Prawdopodobnie widzisz już, że możesz wykorzystać to do **rozszerzania możliwości modelu** oraz **adresowania jego ograniczeń**.

Niewykluczone, że zastanawiasz się, na czym polega różnica pomiędzy zastosowaniem Function Calling w zwykłym generowaniem obiektu JSON, co mieliśmy już okazję robić. Podobnie też udało nam się podłączyć model do zewnętrznych źródeł wiedzy, a nawet stron www.

Otóż poza samym zwiększeniem kontroli nad strukturą, zyskujemy także możliwość **automatycznego wyboru funkcji, która ma zostać wykonana**. To otwiera przed nami koncepcję **Agentów** oraz coś, co określam mianem **Single Entry Point**, czyli jednego punktu wejścia. Inaczej mówiąc, zyskujemy tutaj przestrzeń do częściowej autonomiczności, którą zarysowuje przykład [**14\_agent**](https://github.com/i-am-alice/2nd-devs/blob/main/14_agent/14.ts).

Mamy tutaj do dyspozycji trzy narzędzia: **dodawanie, odejmowanie i mnożenie**, którymi może posługiwać się model. Obecnie GPT-4 świetnie radzi sobie z większością obliczeń związanych z dodawaniem czy odejmowaniem, jednak mnożenie, szczególnie większych liczb, nadal wypada różnie. Ostatecznie najbardziej istotny jest tutaj mechanizm **samodzielnego wybierania narzędzia oraz sposobu posługiwania się nim**.

![](https://assets.circle.so/b882j20xk1f5i02v0lvnw0r2b67z)

W moim przypadku obecna lista narzędzi uwzględnia:

*   Połączenie z listą zadań (Notion / Todoist)
    
*   Połączenie z kalendarzem (Google Calendar)
    
*   Połączenie z aplikacją do notatek (Notion / Obsidian)
    
*   Połączenie z feed readerem (Feedly)
    
*   Połączenie z usługami streamingowymi (Spotify)
    
*   Połączenie z systemem faktur (Fakturownia)
    
*   Połączenie z systemami sprzedaży (Stripe)
    
*   Połączenie z generatorami grafik (mój własny)
    
*   Połączenie z własnymi narzędziami (np. do wyszukiwania w DuckDuckGo czy odczytywaniem treści stron i dokumentów)
    
*   Połączenie bezpośrednio z API Alice (zapamiętywanie / aktualizowanie wspomnień / nauka nowych umiejętności)
    
*   Połączenie z modelem Wolfram Alfa (Conversational API) na potrzeby prywatnych zastosowań związanych z obliczeniami i ogólną wiedzą, np. pogodą
    
*   Połączenie z Google Maps API w kontekście zadań związanych z lokalizacją i odległością pomiędzy nimi
    
*   Dodatkowe integracje do powiadomień w systemie urządzeń Apple
    

Powyższa lista może na ten moment pełnić dla Ciebie rolę inspiracji, ponieważ bliżej przyjrzymy się jej później. Aby jednak nie zostawiać Cię tylko z nią, zaprojektujemy narzędzie umożliwiające zarządzanie **prywatną listą zadań**.

## Projektowanie własnych narzędzi dla LLM

Pierwsze narzędzia dla modelu text-davinci-002 projektowałem już w połowie grudnia 2022, czyli zaraz po premierze ChatGPT. Przez ten czas wykorzystywałem je w różnych kontekstach: **bezpośredniej rozmowie, interakcji głosowej czy automatyzacjach.** Spotkałem po drodze zarówno problemy, jak i interesujące techniki, które zwiększają ich użyteczność.

Przede wszystkim, ogromną rolę odgrywa wspomniana koncepcja "single entry point", czyli "jednego punktu wejścia". Polega ona na tym, aby interakcja z modelem wyposażonym w narzędzia odbywała się możliwie **z poziomu jednego okna czatu, bez konieczności wciskania dodatkowych przycisków czy określenia ustawień**. Oznacza to, że model powinien samodzielnie podejmować decyzję o sposobie wykonania zadania, co już zresztą zobaczyliśmy. Naturalnie odbywa się to kosztem **zmniejszenia kontroli** z naszej strony, więc mogą zdarzyć się sytuacje w których konieczne będzie **bezpośrednie sugerowanie oczekiwanej ścieżki**.

Zdolność do wybierania narzędzi w dużym stopniu zależy od **jakości promptu**, który za to odpowiada. Z tego powodu nie możemy oczekiwać działania takiej mechaniki na poziomie człowieka i musimy określić sobie pewne zasady, które zwiększą prawdopodobieństwo tego, że model **będzie zachowywał się zgodnie z naszymi oczekiwaniami**. Cześć z zachowań możemy także kontrolować programistycznie (np. poprzez walidację niektórych pól czy dodawanie wartości domyślnych).

Wdrożeniem takich mechanik zajmiemy się w kolejnych modułach AI\_Devs. Rysuję Ci jednak kontekst już teraz, aby jasne były dla Ciebie niektóre fakty, które uwzględnimy za chwilę. Naszym celem będzie zbudowanie narzędzia **zdolnego do zarządzania zadaniami** na podstawie przesłanej wiadomości. Rolą modelu będzie jej zinterpretowanie i podjęcie opisanych działań.

Nie będziemy jednak skupiać się na bardzo prostej, bezpośredniej interakcji i zaprojektujemy mechanizm zdolny do **faktycznego zarządzania** zadaniami. Konkretnie mowa tutaj o:

*   Interpretowaniu naszego zapytania i pobraniu z niego danych
    
*   Przygotowaniu integracji do faktycznego wykonania operacji
    
*   Wsparciu dla akcji CRUD - Dodaj, Odczytaj, Zaktualizuj, (“Usuń” pominiemy, bo w tym przypadku usunięcie zadania to zmiana jego statusu na zakończony)
    
*   Możliwości dodania wielu wpisów jednym poleceniem
    
*   Opracowaniu informacji zwrotnej, potwierdzającej wykonanie zadania
    

Podkreślam tutaj **pracę z prywatną listą zadań** ze względu na politykę prywatności. Warto także dopracować całą mechanikę na "testowym koncie" lub początkowo wykluczyć z niej akcje nieodwracalne (np. modyfikowanie i usuwanie).

**Interpretowanie zapytania**

Ogólna koncepcja jest niezwykle prosta i polega na wygenerowaniu obiektu JSON opisującego akcję, na której nam zależy. Następnie z pomocą skryptu (lub automatyzacji) możemy go wykorzystać do wprowadzenia faktycznych zmian w aplikacji do zadań. Mówimy więc tutaj o:

*   akcji dodawania zadań
    
*   akcji aktualizacji zadań
    
*   akcji oznaczania zadań jako zakończone
    
*   akcji umożliwiającej pobranie listy nieskończonych zadań
    

Pełne definicje tych struktur znajdziesz w przykładzie [**15\_todoist**](https://github.com/i-am-alice/2nd-devs/blob/main/15_tasks/15.ts), a jedną z nich, odpowiedzialną za dodawanie zadań, na obrazku poniżej. Mamy więc tutaj **listę zadań** z właściwościami **content** oraz **due\_string** (czas realizacji). Naturalnie możesz swobodnie rozbudować ten obiekt, mając na uwadze limity kontekstu.

![](https://assets.circle.so/l9a18d688si92b06bqasyeb22xpv)

**Podejmowanie działania**

Druga część integracji obejmuje **przygotowanie logiki realizującej działanie dodania zadań**. Aby uczynić tę akcję nieco bardziej elastyczną, zadbałem o to, aby **móc dodawać wiele zadań jednocześnie**. Z kolei w celu **optymalizacji czasu potrzebnego na jego realizację**, zapytania realizowane są **równolegle**.

Naturalnie implementacja będzie różnić się w zależności od technologii, na jaką się zdecydujesz, jednak schemat pozostaje zawsze taki sam, nawet w przypadku narzędzi no-code, czyli:

*   Funkcja lub scenariusz automatyzacji, przyjmujące listę zadań wygenerowaną przez Function Calling na podstawie zapytania użytkownika
    
*   Faktyczny kontakt z API w celu przesłania wszystkich wpisów
    
*   Zwrócenie potwierdzenia, które w kolejnym kroku może być wykorzystane w celu **parafrazy wiadomości** informującej użytkownika o wykonanym zadaniu
    

![](https://assets.circle.so/9ekm9z9sl8w07ebqf1p2hnuntqgs)

  
Przykład prezentujący połączenie wszystkich wymienionych elementów znajdziesz w katalogu [**15\_tasks**](https://github.com/i-am-alice/2nd-devs/tree/main/15_tasks). Ze względu na jego złożoność, przyjrzymy się mu dość szczegółowo, jednak i tak polecam uruchomienie go i przetestowanie na swoim **testowym koncie Todoist**.

**Implementacja**

W pliku [15.ts](https://github.com/i-am-alice/2nd-devs/blob/main/15_tasks/15.ts) zdefiniowałem funkcję **act** odpowiadającą za **interakcję z modelem**. Przekazana do niej wiadomość użytkownika, zostaje przesłana do OpenAI **i jeśli jest poleceniem skojarzonym z jedną z dostępnych akcji**, to dalsza część kodu **uruchamia funkcję łączącą się z Todoist**. Jeśli dojdzie do wykonania akcji, to odpowiedź jest **parafrazowana** lub w przeciwnym razie, zwracam bezpośrednią odpowiedź modelu.

![](https://assets.circle.so/shq53ngr6moh5416uo4654885xj7)

  
Wygląda to tak:

*   User: Cześć!
    
*   AI: Jak mogę Ci pomóc?
    
*   User: Muszę napisać newsletter na jutro, dodaj to do listy
    
*   (AI wykonuje akcję i otrzymuje odpowiedź z API)
    
*   AI: (parafrazowane) Dodałem do Twojej listy napisanie newslettera na jutro
    

Mamy więc do czynienia ze swobodną konwersacją pomiędzy użytkownikiem, a GPT-4 zdolnym do posługiwania się listą zadań!

W pliku **todoist.ts** zdefiniowałem wszystkie funkcje, którymi posługuje się model (jako narzędziami). Mówimy tutaj o prostych interakcjach z API (aby nie komplikować przykładów, pominąłem obsługę błędów).

![](https://assets.circle.so/e90ztoiefp4ytnh03qk7ckayokbg)

No i ostatecznie, w pliku **schema.ts** zapisałem **schematy opisujące funkcje** na potrzeby OpenAI. Możesz je potraktować jako referencję do tworzenia własnych funkcji, posługujących się np. narzędziami do zadań z których korzystasz lub innymi aplikacjami, usługami, czy nawet urządzeniami (podłączonymi do sieci).

![](https://assets.circle.so/0a0fptogwaciktr2r1vm1aymnbdy)

  
**Rezultat**

W głównym pliku [**15.ts**](https://github.com/i-am-alice/2nd-devs/blob/main/15_tasks/15.ts) możemy zdefiniować listę poleceń, które nasz asystent ma zrealizować. Oczywiście, aby faktycznie było to możliwe, konieczne jest podanie swojego klucza API w pliku **.env** znajdującym się w głównym katalogu projektu. Efekt działania skryptu, można zobaczyć na animacji poniżej, lub pod [tym adresem](https://cloud.overment.com/tasks-1695500849.gif).

![](https://assets.circle.so/ocunjkoreiz8e4hhynw628q1ruda)

## Własne narzędzia no-code dla LLM

Pomimo tego, że potrafię programować, często sięgam po rozwiązania no-code w celu **ułatwienia sobie pracy**. Odgrywa to szczególną rolę w przypadku usług, których API nie jest tak dobre, jak Todoist. Poza tym stworzenie scenariusza jest dla mnie zdecydowanie szybsze, niż implementowanie całej logiki samodzielnie. Ostatecznie, narzędzia no-code można także **wykorzystywać równolegle**, ponieważ przykładowo, zamiast definiować funkcje wchodzące w interakcję z API, mogłem stworzyć scenariusz [make.com](http://make.com/), który realizowałby ich zadanie.

W przykładzie [**16\_nocode**](https://github.com/i-am-alice/2nd-devs/tree/main/16_nocode) znajdziesz kod będący alternatywną wersją [**15\_tasks**](https://bravecourses.circle.so/c/lekcje-programu/c02l05-llm-poslugujacy-sie-narzedziami), jednak uwzględnia kilka zmian. (Pamiętaj, że aby go uruchomić, potrzebujesz zaimportować scenariusz [Make.com](http://make.com/) i podłączyć do niego swoje konta. Powiązany z nim adres webhooka umieść kodzie pliku 16.ts w miejscu w którym aktualnie znajduje się pusty adres webhooka).

*   Zamiast rozbicia na pojedyncze funkcje, mamy definicję **tylko jednej, ogólnej, której celem jest zarządzanie zadaniami**. Takie podejście pozwala nam np. na **zrealizowanie wielu zadań jednym zapytaniem**. Możemy więc za jednym razem poprosić zarówno o dopisanie zadań, jak i zakończenie innych, które wymienimy.
    
*   Logika funkcji **w całości przeniesiona jest do** [**make.com**](http://make.com/), przez co **natychmiast mamy do niej zdalny dostęp**.
    

![](https://assets.circle.so/250qcbgndtgbpv7cylkikaop78ou)

*   ⚡ [Pobierz Blueprint](https://cloud.overment.com/manage_tasks-1695501872.json)
    

Powyższy scenariusz może zostać naturalnie rozbity na pojedyncze akcje, jednak w tym przypadku zdecydowałem się uwzględnić je wszystkie i **sterować jego wykonaniem w zależności od wyniku Function Calling**. Gdy zaimportujesz go do Make, dokładnie jak we wcześniejszych przykładach, potrzebujesz podłączyć swoje konta Todoist i OpenAI oraz ewentualnie dostosować prompty.

Zaznaczam w tym miejscu, że scenariusz może być **znacznie prostszy** i odpowiadać wyłącznie za pojedyncze zadanie. Wówczas, nawet jeśli Twoje doświadczenie z [make.com](http://make.com/) jest małe, to stosunkowo łatwo będziesz w stanie połączyć się np. ze Slackiem w celu zintegrowania swojego asystenta (pokażę jak to dokładnie zrobić w dalszej części kursu).

## Podsumowanie narzędzi dla LLM

Tworzenie narzędzi dla LLM polega w dużym stopniu na opracowaniu **logiki** oraz **instrukcji**, którymi model będzie posługiwać się w trakcie konwersacji. Zawsze jednak trzeba mieć na uwadze wszystkie ograniczenia, takie jak limity tokenów czy możliwość halucynacji. Z tego powodu warto unikać **nieodwracalnego wprowadzania zmian** bez ich wcześniejszej weryfikacji.

Na przykładzie naszej listy zadań, moglibyśmy zrobić tak, że model **ma dostęp wyłącznie do jednego projektu**, a pozostałe zadania może np. jedynie odczytywać. Co więcej, moglibyśmy w rękach modelu pozostawić wyłącznie **wybór akcji**, natomiast za resztę operacji odpowiadałaby logika działająca po stronie kodu.

W dalszej części kursu pójdziemy krok dalej i znacznie rozszerzymy wachlarz narzędzi, którymi będzie mógł posługiwać się asystent. Do tego czasu, warto zaprojektować przynajmniej jedną umiejętność AI, która może pomóc nam w codzienności (np. lista zakupów czy zarządzanie domowym budżetem).

* * *

## Zadanie praktyczne

1.  Wygeneruj listę 7 ciekawostek (jedna ciekawostka na linię) na temat podanego miasta. Nie wolno jednak użyć nazwy tego miasta ani w polu SYSTEM, ani w wygenerowanej odpowiedzi. Utrudnieniem jest fakt, że system działa w oparciu o GTP-3.5-turbo. [https://tasks.aidevs.pl/chat/cities](https://tasks.aidevs.pl/chat/cities)
    
2.  Wykonaj zadanie o nazwie **functions** zgodnie ze standardem zgłaszania odpowiedzi opisanym na **tasks.aidevs.pl**. Zadanie polega na zdefiniowaniu funkcji o nazwie addUser, która przyjmuje jako parametr obiekt z właściwościami: imię (**name, string**), nazwisko (**surname, string**) oraz rok urodzenia osoby (**year, integer**). Jako odpowiedź musisz wysłać jedynie ciało funkcji w postaci JSON-a. Jeśli nie wiesz, w jakim formacie przekazać dane, rzuć okiem na hinta: [https://tasks.aidevs.pl/hint/functions](https://tasks.aidevs.pl/hint/functions)  
    

Wskazówka: zadanie 2 w make.com należy wykonać z pomocą modułu JSON. Przy definiowaniu struktury danych, przy ich aktualizowaniu, można wybrać opcję odświeżenia, aby zobaczyć wprowadzone zmiany:

![](https://assets.circle.so/d5zp2uxivgtb1ds967i8xjs4ewwg)