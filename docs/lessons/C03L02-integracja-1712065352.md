[Wersja YouTube](https://youtu.be/1zPq0NraBac)

Podstawowe połączenie LLM z kodem aplikacji nie jest szczególnie wymagające i sprowadza się do prostych zapytań do API. W ten sposób można zbudować imponujący prototyp, który w niektórych sytuacjach może nawet stać się użyteczny dla nas, ale nie będzie nadawał się do oddania w ręce użytkowników.

Dość szybko interakcja z LLM zaczyna ujawniać różnego rodzaju wyzwania, wynikające z niedeterministycznej natury modeli. Do gry wchodzi tutaj również modyfikowanie promptów, korzystanie z różnych modeli (i ich wersji) czy szereg dodatkowych zabezpieczeń i optymalizacji wydajności.

W rezultacie szybko dochodzimy do momentu, w którym konieczne staje się posiadanie nie tylko obszernej wiedzy z zakresu dużych modeli językowych, ale również programowania, które znamy na co dzień i za chwilę przekonamy się, co to dokładnie oznacza.

## Organizacja aplikacji

**Aplikacja, która integruje się z LLM, swoją strukturą do złudzenia przypomina taką, która z AI nie ma nic wspólnego.** Myślę, że dość dobrze obrazuje to repozytorium projektu Quivr, który umożliwia "rozmawianie z danymi", pełniąc funkcję "Second Brain". Wewnątrz projektu widać prawdopodobnie dobrze znane Ci elementy takie jak autoryzacja i uwierzytelnienie połączenia, middleware, ścieżki, repozytoria czy serwisy. Nietrudno zauważyć także logikę odpowiadającą za parsowanie różnych formatów plików czy vector store, no i same testy. Zasadniczo z elementów związanych bezpośrednio z LLM można wyróżnić **połączenie z modelem** i **vector store** oraz ewentualnie **parsery**.

![](https://assets.circle.so/pjoxgei53f8g1cecw064lxi4wa2p)

Skala obecności elementów związanych z LLM może różnić się w zależności od projektu. W przypadku logiki Alice API z której korzystam na swoje potrzeby, udział LLM jest dość duży i uwzględnia:

*   Rozpoznawanie intencji użytkownika
    
*   Autorefleksję oraz wzbogacenie zapytania
    
*   Wyszukiwanie hybrydowe uwzględniające dodatkowe filtrowanie i klasyfikowanie rezultatów
    
*   Planowanie działań uwzględniające wykorzystanie zewnętrznych narzędzi oraz informacji z pamięci długoterminowej
    
*   Zaawansowaną kompozycję promptów oraz ich wykorzystywanie wykraczające poza pojedyncze interakcje (np. łączenie ze sobą różnych promptów i automatyczną weryfikację ich działania)
    
*   Pracę z różnymi formatami treści, uwzględniającą także obrazy i formaty audio
    
*   Dodatkową warstwę zabezpieczeń i optymalizacji, wynikającą z obecności LLM w kodzie aplikacji
    

> Info: (wszystkie wymienione wyżej elementy, których nie omówiliśmy, pojawią się w drugiej połowie AI\_Devs).

Mówimy więc tutaj o sytuacji, w której logika związana z LLM jest **odseparowana od głównej części aplikacji**. Przy odpowiedniej architekturze, może być rozwijana i testowana niezależnie. Wówczas, na przykład, zmiana promptu związanego z mechanizmem autorefleksji będzie wpływać wyłącznie na format zapisywanych informacji, ale logika odpowiadająca za ich faktyczne zapisanie w bazie, pozostanie taka sama.

Przykładowo, gdy projektuję narzędzia dla mojej prywatnej wersji "Alice", to robię to tak, aby stanowiły one niezależną, odseparowaną część kodu. Wówczas pełnią rolę dodatkowych modułów, które mogą zostać włączone lub wyłączone albo zmodyfikowane. Wystarczy tylko, że zadbam o to, aby ich interfejs (format danych wejściowych i wyjściowych) pozostał niezmienny i w miarę możliwości standaryzowany. To sprawia, że z takich narzędzi może korzystać zarówno Alice, jak i sam mogę wywołać je samodzielnie poprzez API.

Obecnie stosowanie LLM w kodzie odbywa się raczej według własnych założeń, ponieważ dopiero zaczynają pojawiać się pierwsze wzorce projektowe. Są one jednak na dość wczesnym etapie i nie wszystkie z proponowanych technik są odpowiednio zweryfikowane i dopasowane do scenariuszy, które sami spotkamy na swojej drodze.

Część z nich nawet już zdążyliśmy omówić lub będziemy je poruszać w dalszych lekcjach. Mowa tutaj o RAG (Retrieval Augmented Generation) czy ReAct (ReAct Prompting). Podobnie też na przykładzie LangChain widzimy, że jego niektóre elementy są niezwykle przydatne, podczas gdy inne bardzo komplikują rozwój aplikacji.

## Organizacja promptów

LangChain oferuje zestaw narzędzi, których założeniem jest **ułatwienie zarządzania strukturą promptów** w kodzie aplikacji. W praktyce wypada to dość różnie i jeśli nie korzystamy z zaawansowanych narzędzi tego frameworka (np. Chain lub Agent), to nie widzę powodu, aby z nich korzystać. W zamian z powodzeniem można pozostać przy np. **Template Strings** i **Tag Functions** obecnych w JavaScript.

Ogólne założenie narzędzi ułatwiających tworzenie szablonów promptów jest zasadne. Zależy na zachowaniu **spójności** i unikania błędów związanych z formatowaniem. Jednak razem z dokładnością w praktyce jest nam potrzebna także **kontrola**, na którą bardzo negatywny wpływ ma **‌(obecnie) mało elastyczna warstwa abstrakcji**.

Idąc dalej, zbudowanie działających promptów nie jest wystarczające, ponieważ musimy zadbać także o to, aby je utrzymywać. Ogólna rekomendacja z którą się spotkałem i która dobrze działa w praktyce polega na tym, aby utrzymywać prompty **możliwie krótkimi** lub stosować format łatwy w edycji (np. prezentowane wcześniej listy Facts & Rules). Wówczas łatwiej także o **wersjonowanie** i obserwowanie tego, w jaki sposób zmiany instrukcji wpływają na zachowanie modelu.

Projektowanie zwięzłych promptów nierzadko wymaga dużej kreatywności oraz nieszablonowego podejścia. Przykładowo, zamiast opisywać szczegółowo to, na czym mi zależy, mogę to pokazać. Podobną technikę stosowaliśmy już wcześniej, zmieniając narrację promptu z "Jesteś \[xyz\] (...)" na "Cześć! Jestem \[xyz\] (...)", która pozwalała na znacznie bardziej precyzyjne oraz zwięzłe układanie instrukcji. Całość obrazuje poniższy przykład. Zwróć uwagę, że praktycznie nie pozostawiam w nim miejsca na dodatkowe komentarze ze strony modelu. Co więcej, jeden z przykładów **zawiera treść która jest instrukcją / pytaniem**, dzięki czemu podkreślam oczekiwane zachowanie nawet w sytuacji, gdy dane będą zawierać coś, co model może zinterpretować jako polecenie.

![](https://assets.circle.so/ju7mq9yw8oe92ngaitkacae0bz40)

Taki prompt składający się z bardzo obrazowych przykładów działa także świetnie w połączeniu z modelem GPT-3.5-Turbo. Jednak mając na uwadze fakt, że **miewa on problemy z podążaniem za instrukcją systemową**, rozpoczynam konwersację dopiero od trzeciej wiadomości, przez co pierwszy fragment dodatkowo wpływa na zachowanie modelu.

Poniższy zrzut ekranu tego nie obrazu, jednak pierwsza wiadomość "hey there!" oraz odpowiedź "{"content": "hey there!"} zostały przeze mnie napisane ręcznie. Dopiero wiadomość "Write the newsletter pls" stanowi faktyczną część konwersacji.

![](https://assets.circle.so/atilrrvaaho1gau2orvluhipz9dv)

*   ⚡ [Zobacz przykład](https://platform.openai.com/playground/p/3dKC6V4vu0RoMYER4NxwZSMR?model=gpt-3.5-turbo-16k&mode=chat)
    

Obecnie wiele z modeli specjalizuje się w generowaniu obiektów JSON i całkiem dobrze radzi sobie z zadaniami polegającymi na ich tworzeniu na podstawie aktualnej konwersacji. W zależności od tego jak trudne jest dobranie odpowiedniej treści generowanego obiektu, warto rozważyć wykorzystanie mniejszych, szybszych i tańszych modeli (np. Haiku lub GPT-3.5-turbo).

Natomiast obecnie najlepszym wyborem w takiej sytuacji staje się GPT-3.5-Turbo-0125 (lub Haiku od Anthropic). Jest to wersja, która poradzi sobie z tak prostym generowaniem obiektu JSON i w przeciwieństwie do swojej starszej wersji, skuteczniej podąża za instrukcjami. Tym bardziej że mamy do dyspozycji nie tylko zdolności samych modeli, ale także omawiany już **json\_mode**

Ważne: właściwość **json\_object** obecnie nie jest jeszcze dostępna w Playground, lecz bezpośrednio w API.

Integrując LLM z kodem, praktycznie w każdym przypadku mówimy o jakimś rodzaju **dynamicznych danych** lub nawet **dynamicznego budowania struktury promptów** z mniejszych części. Wiemy już, że pomimo tego, że modele dobrze radzą sobie z nieustrukturyzowanymi danymi, to tam, gdzie potrzebujemy precyzji, należy dbać o spójność. W przypadku dynamicznych promptów kluczowa jest ich **modularność** (możliwość łączenia ze sobą fragmentów w różnych konfiguracjach) oraz **separatory**. Np. stosowanie "###" nie sprawdzi się dla treści zapisanej jako treść Markdown, ponieważ taki ciąg znaków może wystąpić bezpośrednio w tekście jako nagłówek H3. Podobnie też, jeśli mamy **zagnieżdżone konteksty**, to chcemy oddzielić je **różnymi rodzajami separatorów**, aby zredukować ryzyko pomylenia treści.

Niezależnie od tego, w jaki sposób zaprojektujesz i zorganizujesz prompty w swojej aplikacji, **krytyczne** jest zadbanie o **pełne monitorowanie**. Samo zapisanie **zapytania** oraz **odpowiedzi** jest niewystarczające, ponieważ nie da Ci pełnych informacji o tym, co się wydarzyło. Brak tych danych, **znacznie utrudni** debugowanie. Jeśli korzystasz z LangChain, to najlepszym sposobem będzie skorzystanie z LangSmith, ponieważ podłączenie polega na utworzeniu projektu i dodania zmiennych środowiskowych (klucz API LangChain pobierzesz w **LangSmith** (jest to mylące)).

![](https://assets.circle.so/5581rc2yh310d85a8hx7onyu815i)

Podstawowe monitorowanie jednak nie wystarczy, ponieważ do LangSmith trafiają jedynie informacje związane z samą interakcją z LLM. Natomiast przepływ danych w Twojej aplikacji, niemal zawsze będzie uwzględniał kontakt z zewnętrznymi źródłami danych oraz różnymi obszarami jej logiki.

Oczywiście nie traktuj tego, jak coś **absolutnie koniecznego** nawet dla prostych projektów, ponieważ mówimy teraz raczej o środowisku produkcyjnym. Gdy będziesz integrować LLM na własne potrzeby, to wystarczy Ci zapisanie **całej konwersacji**, co dzieje się w LangSmith.

## Walidacja danych wejściowych i wyjściowych

Walidacja danych na potrzeby LLM uwzględnia wszystko to, **co już znasz z klasycznych aplikacji** i dodaje kilka dodatkowych aspektów. Jeden z nich dotyczy faktu, że gdy użytkownik ma **dowolność aplikacji**, musimy przygotować się na możliwie każdą sytuację. Wówczas najlepszym sposobem "obrony" jest zastosowanie tych samych narzędzi, czyli LLM.

W jednej z moich aplikacji stosuję zestaw **zasad**, których ma przestrzegać AI. Zasady te zestawiane są z **konwersacją** i jeśli jakaś z zasad jest naruszona, otrzymuję odpowiedź, która blokuje dalszą interakcję. Akurat w tym przypadku **pomijam najnowszą odpowiedź generowaną przez model**, jednak jeśli nie korzystasz z opcji streamingu, to ją także możesz walidować.

![](https://assets.circle.so/uoq4opaxwjs50a0n63gp037p02ao)

Sam korzystam z bardzo zbliżonego formatu (ale rozszerzonej listy zasad) do weryfikacji zapytań kierowanych do mojej aplikacji. Przykład poniżej pokazuje to w jaki sposób model przestrzega określonych przeze mnie zasad, odmawiając udzielania odpowiedzi na tematy spoza swojego zakresu.

![](https://assets.circle.so/p8d58hvaiufqsp6n4240juevmeid)

Jednak czasem zdarzają się sytuacje w których normalne wiadomości także są traktowane jako zagrożenie. **Jestem w stanie to zauważyć, dzięki automatycznemu flagowaniu takich wątków**. Jedna z konwersacji uwzględniała pytanie, którego nie przewidziałem, ponieważ moje zasady informowały model o ewentualnej chęci naruszenia zabezpieczeń, w związku z tym zablokował pytanie o **prompt injection**. I fakt, Prompt Injection to technika ataku, jednak zapytanie o nią w kontekście kursu o Prompt Engineeringu powinno być dopuszczalne.

![](https://assets.circle.so/3i3ra4l2xos5vcoc06sn8bf8fsfr)

Pokazuję Ci to, aby zasygnalizować, że **obecnie trudno bronić się przed atakami**, ponieważ nawet jeśli uda nam się dobrze zabezpieczyć, to i przypadkowo możemy sprawić, że użytkownicy niemający na celu nic złego, zostaną zablokowani.

Jednak z drugiej strony, przypadek o którym teraz mówię, dotyczy ~1% użytkowników, więc nadal mówimy tutaj o wysokiej skuteczności, tym bardziej że system zablokował 100% niepożądanych zachowań. Oczywiście nie ma gwarancji, że tak pozostanie, jednak w kontekście mojego projektu, można uznać, że system spełnia swoje zadanie.

Przy takiej walidacji danych istotną rolę odgrywa także **słowo kluczowe**, które w tym przypadku zostało ustawione na **ALLOW** lub **BLOCK**. Nawet jeżeli w konwersacji znajdzie się prompt zaburzający działanie modelu, to i tak **dalsza logika aplikacji nie dopuści do dalszej interakcji**. Co więcej, mówimy tutaj o **oddzielnym prompcie**, więc przekazanie tych słów do oryginalnej konwersacji również nie wchodzi w grę. No i ostatecznie same słowa kluczowe są **krótkie**, przez co ich generowanie nie zajmuje zbyt wiele czasu.

## Parsowanie odpowiedzi modelu i obsługa błędów

Integracja kodu aplikacji z LLM wymaga wymiany informacji pomiędzy kodem a modelem. To sprawia, że konieczne jest utrzymanie kontroli nad strukturą takiej komunikacji. Choć o tym już wspominałem, to zwracam uwagę na to, aby wykorzystać w tym celu obiekty JSON i **json\_mode**. To jednak nie wszystko.

Podobnie jak w przypadku walidacji, parsowanie również może odbywać się poprzez interakcję z modelem. Naturalnie nie zawsze jest to wymagane, ponieważ czasem wystarczy nam zwykłe wyrażenie regularne lub jakiś rodzaj transformacji tekstu.

W jednej z wersji Alice, gdy jeszcze nie miałem dostępu do Function Calling, w celu pobierania obiektu **prostego obiektu JSON** z odpowiedzi modelu, wykorzystywałem poniższą funkcję. Jej zadaniem było odnalezienie nawiasów klamrowych oraz podjęcie próby parsowania tekstu znajdującego się pomiędzy nimi. Oczywiście należy podkreślić, że ten kod działał tylko w przypadku gdy wiadomość **nie zawierała kilku obiektów**. To jednak w tym przypadku nigdy nie miało miejsca, a jeśli nawet by się wydarzyło, to informacja o błędzie w pełni mnie satysfakcjonowała.

![](https://assets.circle.so/0d7d3um9qgy7z78kezhane7xkda1)

  
Zakładając jednak, że **nie mam pewności** czy model zwróci mi pożądany format (bo niekoniecznie musi to być JSON) i jednocześnie **nie mam możliwości** wykryć go programistycznie, to wówczas mogę skorzystać z modelu. Co więcej, nie muszę tego robić od razu, ponieważ mogę podjąć próbę parsowania go z pomocą kodu. Jeśli próba się nie powiedzie, zostanie wyrzucony wyjątek, który mogę przechwycić i zamiast informować o tym fakcie użytkownika, mogę przekazać zapytanie **do mocniejszego modelu** (lub innego promptu), aby poprawił działanie słabszego modelu.

  

![](https://assets.circle.so/83589m53vohmx290htikcjpid14h)

Przykład **bardzo prostej** implementacji takiego mechanizmu znajdziesz w [**20\_catch**](https://github.com/i-am-alice/2nd-devs/blob/main/20_catch/20.ts). Domyślnie korzystam tam z modelu **gpt-3.5-turbo**, dzięki czemu oszczędzam czas i pieniądze. Jednak mój prompt nie jest zbyt dobrze napisany (celowo) i w niektórych przypadkach nie potrafi podążać za początkową instrukcją. W rezultacie zwrócona odpowiedź **nie zawiera obiektu JSON** i psuje działanie mojego skryptu.

![](https://assets.circle.so/o25z3p5hou5qfaqqo70h56u6lahy)

Gdy taka sytuacja ma miejsce, do gry wchodzi model GPT-4, który **naprawia ten błąd**. Oczywiście nie mam 100% pewności, że automatyczne naprawianie się uda, jednak jednocześnie zwiększam stabilność aplikacji. W produkcyjnym zastosowaniu musiałbym jeszcze obsłużyć sytuację w której GPT-4 także sobie nie poradzi, jednak na potrzeby tego przykładu, nie ma takiej potrzeby.

Alternatywnie mógłbym spróbować skorzystać **z tego samego modelu**, ale ze zmodyfikowanym promptem, którego zadaniem byłoby **ocenienie i naprawienie** wyniku, uwzględniając zarówno dane przekazane przez użytkownika, jak i **pierwotną odpowiedź modelu**. Rodzaj wybranej strategii będzie zależał od konkretnego przypadku i po prostu warto wziąć pod uwagę różne opcje. Ostatecznie jednak mechanizmy związane z próbą **automatycznego naprawienia** czy **parsowania danych z pomocą LLM** powinny być wykorzystywane awaryjnie, dlatego warto popracować nad optymalizacją promptów, by te mogły spełniać swoją rolę za pierwszym razem.

## Powiadomienia o błędach

Z jakiegoś powodu dokumentacja OpenAI API nie zawiera informacji na temat statusów błędów. Co więcej, o problemach jesteśmy informowani zarówno przez statusy błędów, jak i wewnętrzne **status\_code** a niekiedy **type**. Oznacza to, że brakuje tutaj spójności i bardzo trudno jest zgromadzić wszystkie informacje o błędach w jednym miejscu, tym bardziej że w sieci znajdują się nieprawdziwe lub niepoprawne wyniki.

Przykładowa informacja na temat błędu wygląda tak:

![](https://assets.circle.so/iekkqnj4u1ug8t0lwxn2rc19crgp)

Natomiast pełna lista błędów z którymi sam się spotkałem, wygląda następująco:

*   **nieprawidłowe zapytanie** status: 400, type: invalid\_request\_error
    
*   **przekroczony limit kontekstu** status: 400, type: invalid\_request\_error
    
*   **przekroczony limit zapytań**: status: 429, code: rate\_limit\_error
    
*   **błąd uwierzytelniania**: status: 401, code: invalid\_api\_key, type: invalid\_request\_error
    
*   **nie znaleziono**: status: 404, code: not\_found\_error
    
*   **błąd uprawnień**: status: 403, code: permission\_error
    
*   **nieprawidłowy klucz API**: status: 400, code: invalid\_api\_key
    
*   **model nieznaleziony (lub brak dostępu)**: status: 404, code: model\_not\_found
    
*   **niewystarczający limit tokenów**: status: 403, code: insufficient\_quota
    
*   **niedozwolona metoda**: status 405, code: method\_not\_supported, type: invalid\_request\_error
    
*   **błąd API OpenAI**: status: 500, code: api\_error
    
*   **błąd serwera**: status: 500, code: server\_error
    

Warto zauważyć, że błąd 429 pojawia się także w sytuacji, gdy **serwer jest obciążony zbyt dużą ilością zapytań**, niekoniecznie wykonywanych tylko przez nas. Choć taka sytuacja ma miejsce bardzo rzadko, warto mieć to na uwadze. Dodatkowo dobrą praktyką jest dodanie **generycznego błędu** informującego o statusie, którego nie ma na liście, ale może zostać dodany przez OpenAI w przyszłości.

## Doświadczenia z LLM "na produkcji"

[Wersja na YouTube](https://youtu.be/3CZiZEsYEM4)

Podczas pracy nad własną wersją Alice, aplikacją desktopową, asystentem eduweb czy szeregiem różnych narzędzi, spotkałem różne sytuacje i wyciągałem z nich wnioski. Oto lista:

*   **Błędy API:** Wykonując dowolne zadanie z LLM, **zakładam, że się nie uda**, głównie ze względu na błędy związane z API. Pomimo tego, że występują obecnie stosunkowo rzadko, to realizowanie **złożonych akcji** wymaga budowania mechanizmów umożliwiających ich wznawianie. W przeciwnym razie generujemy niepotrzebne koszty i marnujemy czas. Głównymi błędami są 429 (niezwiązane z moją aktywnością) oraz błąd 500.
    
*   **Limity:** Limit kontekstu, limit zapytań, limit przetwarzanych tokenów, twardy limit kosztów. Każdy z nich pojawia się co jakiś czas w moich aplikacjach i każdy wymaga dodatkowej uwagi, szczególnie w zastosowaniach produkcyjnych. Im większa skala, tym większa potrzeba wprowadzania systemu kolejek lub innych technik optymalizacji zarówno po stronie kodu, jak i LLM. Same limity API ostatnio zostały powiększone i rozłożone na tzw. tiery. Ich aktualne wartości, znajdziesz [tutaj](https://platform.openai.com/docs/guides/rate-limits/usage-tiers?context=tier-free).
    

*   **Niespodziewane koszty:** Estymacja kosztów bywa dość zdradliwa, szczególnie w systemach, które budują kontekst dynamicznie. Moje największe koszty były związane z tym, że prompt sprawdzał się na danych testowych, ale z różnych powodów generował zbyt dużo błędów na całej bazie. Inny przypadek uwzględniał **nieprawidłowe zapisywanie metadanych**, co pozornie wyglądało w porządku, ale całkowicie nie sprawdziło się w praktyce. Dobrze więc kilkukrotnie przekonać się na różnorodnych zestawach danych, czy nasze mechanizmy działają w porządku.
    
*   **Czas odpowiedzi:** Zwykle połączenie HTTP kończone jest po około 30-120s po braku odpowiedzi. W niektórych przypadkach możemy na to jakoś wpływać, a w innych nie. W sytuacji, gdy zwiększanie limitu nie wchodzi w grę, należy rozważyć system kolejkowania, pollingu czy webhooków. Koniecznie też należy wziąć pod uwagę korzystanie z najnowszych wersji modeli GPT-4 Turbo (gpt-4-preview) oraz gpt-3.5-turbo-0125.
    
*   **Nieoczekiwane zachowanie:** Raz na jakiś czas w moich systemach pojawiają się błędy, które również poprawiają się same. Niekiedy zidentyfikowanie ich źródła jest bardzo trudne, ponieważ nie można ich powtórzyć. Są to bardzo rzadkie sytuacje, ale jednak mają miejsce i przypominają mi o konieczności monitorowania zachowania mojego systemu oraz obsłudze błędów.
    
*   **Różnica językowa:** Obecnie w 100% pracuję z LLM w języku angielskim. Oczywiście można przygotować interakcję w języku polskim. Różnicy nauczyłem się w dość nietypowy sposób, bo poprzez **różnice językowe embeddingu** i problemach z wyszukiwaniem anglojęzycznej bazy zapytaniami zapisanymi w języku polskim. Teoretycznie różnica w statystykach nie była zbyt duża, ale w przypadku koniecznej precyzji, okazała się znacząca.
    
*   **Szybkość działania:** Na szybkość generowania odpowiedzi ma wpływ wiele czynników. W przypadku bardziej złożonych systemów wprost kluczowe staje się równoległe realizowanie zapytań oraz możliwie unikanie stosowania modeli dla zadań, które tego absolutnie nie wymagają. Przykładowo dla prostych wyszukiwań znacznie lepiej sprawdza się klasyczny silnik wyszukiwania, niż bazy wektorowe wymagające embeddingu, który wydłuża czas generowania odpowiedzi. Z kolei w innych sytuacjach embedding i bazy wektorowe sprawdzą się lepiej niż stosowanie LLM (np. gdy porównujesz jakieś dane, zamiast sięgać po LLM, możesz skorzystać z Similarity Search, co może okazać się bardziej wydajne)
    
*   **Skala pamięci:** Stworzenie systemu RAG (lub podobnego) na małej skali danych jest banalnie proste, szczególnie gdy mamy do dyspozycji obecne narzędzia oraz kontekst modeli na poziomie 128 tysięcy tokenów. Projektowanie **działającego** systemu pracującego na dużych zestawach informacji wymaga wiele pracy oraz dużego doświadczenia w przetwarzaniu danych. Dodatkowo niemal 90% aktywności związana jest z klasycznym programowaniem, a nie interakcją z LLM, chociaż wiedza na temat modeli jest krytyczna do tego, aby zwyczajnie wiedzieć, co należy zrobić.
    
*   **Cache/Szybszy dostęp do danych**: Stosowanie jakiejkolwiek techniki przyspieszającej dostęp do informacji jest zdecydowanie warte rozważenia. Pierwszym powodem jest szybkość działania systemu, drugą koszty. W przypadku nawet stosunkowo małych systemów, ale pracujących na dużych zestawach danych, rachunki bardzo szybko rosną. Kojarzenie ze sobą podobnych zapytań lub stosowanie programistycznych technik dostępu do danych, zamiast ich przetwarzania przez model, jest zawsze bardzo wskazane.
    

Prawdopodobnie mógłbym wymieniać w ten sposób jeszcze kilkanaście różnych punktów. Myślę jednak, że powyższe podkreślają najbardziej istotne wątki, które mogą przysporzyć Ci pracy lub wygenerować niepotrzebne koszty czy negatywnie wpłynąć na stabilność Twojej aplikacji.

* * *

## Zadania praktyczne

1.  Rozwiąż zadanie z API o nazwie "**scraper**". Otrzymasz z API link do artykułu (format TXT), który zawiera pewną wiedzę, oraz pytanie dotyczące otrzymanego tekstu. Twoim zadaniem jest udzielenie odpowiedzi na podstawie artykułu. Trudność polega tutaj na tym, że serwer z artykułami działa naprawdę kiepsko — w losowych momentach zwraca błędy typu "error 500", czasami odpowiada bardzo wolno na Twoje zapytania, a do tego serwer odcina dostęp nieznanym przeglądarkom internetowym. Twoja aplikacja musi obsłużyć każdy z napotkanych błędów. Pamiętaj, że pytania, jak i teksty źródłowe, są losowe, więc nie zakładaj, że uruchamiając aplikację kilka razy, za każdym razem zapytamy Cię o to samo i będziemy pracować na tym samym artykule.
    
2.  Napisz konwerter staroafrykańskiego języka znaczników na kod HTML. Musisz poinstruować GPT-3.5-turbo, jak należy obchodzić się z tym kodem i co on oznacza. [https://tasks.aidevs.pl/chat/format](https://tasks.aidevs.pl/chat/format)  