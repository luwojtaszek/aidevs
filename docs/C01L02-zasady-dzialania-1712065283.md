Åatwo spotkaÄ‡ siÄ™ z opiniÄ…, Å¼e **nie wiemy, jak LLMy dziaÅ‚ajÄ…**. Do pewnego stopnia jest to prawda, ale nie do koÅ„ca, poniewaÅ¼ oczywiÅ›cie posiadamy wiedzÄ™ na temat ich projektowania, trenowania oraz rozwijania. Po prostu nie jest do koÅ„ca jasne, **co dokÅ‚adnie** dzieje siÄ™ pomiÄ™dzy przesÅ‚aniem wiadomoÅ›ci a otrzymaniem odpowiedzi.

Niebawem stanie siÄ™ dla Ciebie bardziej zrozumiaÅ‚e, dlaczego taka sytuacja ma miejsce oraz to, w jaki sposÃ³b moÅ¼emy wykorzystaÄ‡ jÄ… do pracy z modelami. Aby daÄ‡ Ci peÅ‚ny obraz, w pierwszej kolejnoÅ›ci przyjrzymy siÄ™ modelom jÄ™zykowym nieco bliÅ¼ej.

## Sieci Neuronowe

Jak wiesz, **funkcje przyjmujÄ… dane (wejÅ›cie) i zwracajÄ… wynik (wyjÅ›cie) dopasowany do nich, uzaleÅ¼niony od ich definicji**. KorzystajÄ…c z nich, moÅ¼emy rozwiÄ…zywaÄ‡ rÃ³Å¼ne problemy, poprzez **kontrolowanie przepÅ‚ywu danych oraz sposobu ich transformacji** w celu osiÄ…gniÄ™cia poÅ¼Ä…danego rezultatu. PrzykÅ‚adem moÅ¼e byÄ‡ prosta funkcja mnoÅ¼Ä…ca przekazanÄ… do niej liczbÄ™ razy dwa.

![](https://assets.circle.so/4lp9j8mbxhak8yjrv71vx4ipcj34)

  
Wraz ze wzrostem zÅ‚oÅ¼onoÅ›ci rozwiÄ…zywanego problemu, zazwyczaj roÅ›nie rÃ³wnieÅ¼ zÅ‚oÅ¼onoÅ›Ä‡ definicji funkcji, aÅ¼ do momentu, w ktÃ³rym implementacja staje siÄ™ zbyt skomplikowana lub wrÄ™cz niemoÅ¼liwa do zrealizowania. Takie sytuacje mogÄ… mieÄ‡ miejsce nawet dla stosunkowo prostych dla czÅ‚owieka zadaÅ„, ktÃ³rych przeÅ‚oÅ¼enie na kod nie jest oczywiste. PrzykÅ‚adem moÅ¼e byÄ‡ **rozpoznawanie obiektÃ³w, dÅºwiÄ™kÃ³w** czy **posÅ‚ugiwanie siÄ™ naturalnym jÄ™zykiem (NLP â€” natural language processing)**. UwzglÄ™dnienie wszystkich zmiennych biorÄ…cych udziaÅ‚ w takich procesach **wymaga zaprojektowania matematycznego modelu reprezentujÄ…cego ich realizacjÄ™ w uproszczony sposÃ³b**.

Aby Ci to lepiej zobrazowaÄ‡, przygotowaÅ‚em prosty kod JavaScript ([zobacz gist](https://gist.github.com/iceener/2626f66078f8d52d72448e1663e577d9)), ktÃ³ry rysuje wykres funkcji falowej. W tym przypadku znamy dokÅ‚adnie zasady rysowania takiego wykresu i jesteÅ›my w stanie rysowaÄ‡ jego kolejne fragmenty poprzez **obliczanie kolejnych punktÃ³w**.

![](https://assets.circle.so/7jp4x488hrt0iwh5rj1otro07pik)

ZakÅ‚adajÄ…c jednak, Å¼e nie znamy tych zasad, musielibyÅ›my zaprojektowaÄ‡ model, ktÃ³ry w uproszczony sposÃ³b realizowaÅ‚by zadanie polegajÄ…ce na rysowaniu wykresu. PoczÄ…tkowo **jego parametry** byÅ‚yby raczej losowe i taki teÅ¼ byÅ‚by wynik jego dziaÅ‚ania. Obrazek poniÅ¼ej pokazuje, Å¼e nasz model nie dziaÅ‚a.

![](https://assets.circle.so/d3oyk2pf3g6dlt2zck45uad26h9s)

JeÅ¼eli jednak zaczniemy modyfikowaÄ‡ niektÃ³re parametry, z chaosu zacznie wyÅ‚aniaÄ‡ siÄ™ pewien ksztaÅ‚t przypominajÄ…cy wykres funkcji falowej. Nie jest to rezultat, ktÃ³rego oczekujemy, jednak idziemy w dobrym kierunku.

![](https://assets.circle.so/rb5xvp69a8e538de6c22atvnizsx)

Konieczne jest dalsze dostosowanie parametrÃ³w, jednak **tym razem wprowadzane zmiany nie mogÄ… byÄ‡ juÅ¼ tak drastyczne, poniewaÅ¼ niektÃ³re z czerwonych punktÃ³w znajdujÄ… siÄ™ we wÅ‚aÅ›ciwych miejscach**. Dobrze byÅ‚oby, gdyby tak pozostaÅ‚o. Uzyskany rezultat bez wÄ…tpienia przypomina wykres funkcji falowej.

![](https://assets.circle.so/gkbcmzwlo9bej14m5ui9o0snyaef)

JeÅ›li teraz naÅ‚oÅ¼ymy nasz wynik na poczÄ…tkowy wykres, zobaczymy, Å¼e niemal wszystkie punkty znajdujÄ… siÄ™ na biaÅ‚ej linii. JednoczeÅ›nie widoczne sÄ… takÅ¼e punkty, ktÃ³re znajdujÄ… siÄ™ delikatnie poza liniÄ…. **Nawet jeÅ›li kontynuowalibyÅ›my dostosowywanie parametrÃ³w, nie osiÄ…gniemy 100% dopasowania i bÄ™dziemy poruszaÄ‡ siÄ™ w obszarze prawdopodobieÅ„stwa.** Co wiÄ™cej, na pewnym etapie trenowanie staje siÄ™Â nieefektywne w kontekÅ›cie samego treningu, jak i pÃ³Åºniejszego dziaÅ‚ania modelu.

![](https://assets.circle.so/95unve0fjh5gzlpz8sjp7icbw5s4)

Akurat w przypadku powyÅ¼szych przykÅ‚adÃ³w, zmiany parametrÃ³w sÄ… bardzo proste i z powodzeniem moÅ¼na wprowadzaÄ‡ je rÄ™cznie. ZresztÄ…, moÅ¼esz sprÃ³bowaÄ‡ samodzielnie, modyfikujÄ…c zmiennÄ… **randomness** [w kodzie dostÄ™pnym tutaj](https://codepen.io/iceener/pen/VwqpoOO?editors=0010) (wartoÅ›ci bliÅ¼sze 0 przybliÅ¼Ä… punkty do wykresu funkcji).

Dla bardziej skomplikowanych procesÃ³w, takich jak np. rozumienie i generowanie jÄ™zyka naturalnego, liczba parametrÃ³w i wariantÃ³w ich ustawieÅ„ drastycznie roÅ›nie. Nie znamy takÅ¼e definicji funkcji, ktÃ³re sÄ… w stanie opisaÄ‡ taki proces. WÃ³wczas konieczne staje siÄ™ zastosowanie automatycznych mechanizmÃ³w zdolnych do ksztaÅ‚towania modelu tak, aby staÅ‚ siÄ™ moÅ¼liwie dokÅ‚adny i jednoczeÅ›nie wydajny. PrzykÅ‚adem takiego mechanizmu sÄ… sieci neuronowe, inspirowane ludzkim mÃ³zgiem.

Bez wchodzenia w szczegÃ³Å‚y dotyczÄ…ce sieci neuronowych, **mÃ³wimy tutaj o strukturze zdolnej do dostosowania siÄ™ do realizacji rÃ³Å¼nych zadaÅ„.** Jednak, aby to byÅ‚o moÅ¼liwe, konieczny jest **trening polegajÄ…cy na dostosowaniu jej parametrÃ³w.** Zwykle odbywa siÄ™ to poprzez ogromne iloÅ›ci powtÃ³rzeÅ„ przeprowadzonych na zestawach danych treningowych w postaci wejÅ›cia (input) i wyjÅ›cia (output) \[istniejÄ… takÅ¼e inne formy treningu\]. Zadaniem sieci jest stopniowe dostrajanie swoich ustawieÅ„ tak, aby dla dostarczonej informacji generowaÄ‡ dopasowany do niej wynik.

## DuÅ¼e Modele JÄ™zykowe

DuÅ¼e modele jÄ™zykowe, takie jak GPT-4, wykorzystujÄ… sieci neuronowe do przetwarzania jÄ™zyka naturalnego. **W prostych sÅ‚owach â€” uczÄ… siÄ™ mÃ³wiÄ‡.** Co wiÄ™cej, u podstaw treningu sieci do realizacji takiego zadania leÅ¼y absurdalnie prosta idea **przewidywania kolejnego fragmentu wypowiedzi** poprzez nieustanne odpowiadanie na pytanie: **"BiorÄ…c pod uwagÄ™ ten tekst, co powinno byÄ‡ dalej?"**.

**Tokenizacja**

Projektowanie modeli jÄ™zykowych wymaga zamiany tekstu na ich reprezentacjÄ™ liczbowÄ…. Obecnie najpopularniejszÄ… strategiÄ… jest tzw. **subword tokenization**, czyli zapisywanie za pomocÄ… liczb fragmentÃ³w sÅ‚Ã³w. Podczas generowania odpowiedzi, model generuje kolejne tokeny (fragmenty sÅ‚Ã³w), odpowiadajÄ…c na wspomniane wyÅ¼ej pytanie, ktÃ³re w takiej sytuacji brzmi: "BiorÄ…c pod uwagÄ™ dotychczasowy tekst, **jaki token stanowi jego kontynuacjÄ™?**".

Bardzo wyraÅºnie moÅ¼emy to zaobserwowaÄ‡ na tej stronie: [platform.openai.com/tokenizer](https://platform.openai.com/tokenizer) lub [https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app/). Kolejne tokeny zostaÅ‚y tutaj wyrÃ³Å¼nione kolorami i wyraÅºnie widaÄ‡, Å¼e np. sÅ‚owo "overment" skÅ‚ada siÄ™ z dwÃ³ch tokenÃ³w: "over" i "ment", ktÃ³re model zamienia sobie na liczby. MoÅ¼na wiÄ™c powiedzieÄ‡, Å¼e np. GPT-4 wÅ‚aÅ›nie tak widzi tekst.

![](https://assets.circle.so/eoaw3wktx498jwpnjnh798xqlcb1)

Decyzja o tym, ktÃ³re fragmenty sÅ‚Ã³w stajÄ… siÄ™ tokenami, zaleÅ¼y od algorytmu tokenizacji oraz zestawu danych, na ktÃ³rych pracujemy. W ten sposÃ³b powstaje sÅ‚ownik (eng. vocabulary), stanowiÄ…cy element modelu jÄ™zykowego. **GÅ‚Ã³wnym jÄ™zykiem dla modeli GPT jest angielski, dlatego tokeny dobierane sÄ… w taki sposÃ³b, aby moÅ¼liwie efektywnie wykorzystywaÄ‡ je podczas generowania treÅ›ci.** Samo wygenerowanie sÅ‚ownika nie jest jednak wystarczajÄ…ce, poniewaÅ¼ konieczne jest takÅ¼e uwzglÄ™dnienie dodatkowych informacji, takich jak chociaÅ¼by znaczenie sÅ‚Ã³w, ktÃ³re takÅ¼e naleÅ¼y zamieniÄ‡ na zestawy liczb, ktÃ³re w tym przypadku nazywamy embeddingiem.

**Embedding**

Embedding to proces podobny do tokenizacji, poniewaÅ¼ takÅ¼e polega na reprezentowaniu sÅ‚Ã³w za pomocÄ… liczb, a konkretnie tablicy liczb, czyli tzw. wektorÃ³w. Modele jÄ™zykowe wykorzystujÄ… tzw. "word embedding", ktÃ³ry umoÅ¼liwia im "rozumienie" znaczenia sÅ‚Ã³w, co jest wykorzystywane miÄ™dzy innymi na etapie generowania odpowiedzi.

JeÅ¼eli weÅºmiemy teraz trzy sÅ‚owa: samochÃ³d, motocykl oraz laptop, to jako ludzie wiemy, Å¼e pierwsze dwa sÄ… do siebie zbliÅ¼one znaczeniem (opisujÄ… pojazd). Embedding dÄ…Å¼y do opisania tej zaleÅ¼noÅ›ci za pomocÄ… liczb, co obrazuje poniÅ¼szy, uproszczony przykÅ‚ad. JeÅ›li porÃ³wnasz wartoÅ›ci liczbowe, zauwaÅ¼ysz podobieÅ„stwo, o ktÃ³rym mÃ³wiÄ™. W praktyce jednak oddanie znaczenia sÅ‚Ã³w jest znacznie bardziej zÅ‚oÅ¼one i iloÅ›Ä‡ wymiarÃ³w wektora stworzonego podczas procesu embeddingu zaleÅ¼na jest od uÅ¼ytego modelu.

OpenAI aktualnie oferuje trzy modele:

*   text-embedding-ada-002 = 1536 wymiarÃ³w
    
*   text-embedding-3-small = 1536 wymiarÃ³w
    
*   text-embedding-3-large = 3072 wymiary
    

UWAGA: nawet jeÅ›li dwa modele majÄ…Â takÄ…Â samÄ… liczbÄ™Â wymiarÃ³w, to nie oznacza to, Å¼e sÄ… ze sobÄ… jakkolwiek kompatybilne.

![](https://assets.circle.so/xrvmeftnyuvfy33cl409slgzi555)

  
Drugim rodzajem embeddingu, z ktÃ³rym mamy do czynienia przy okazji LLM, jest tzw. "sentence embedding", ktÃ³ry, jak nazwa wskazuje, oddaje znaczenie dÅ‚uÅ¼szych treÅ›ci. WÃ³wczas uwzglÄ™dnione jest nie tylko znaczenie sÅ‚Ã³w, ale takÅ¼e kontekst, w ktÃ³rym zostaÅ‚y uÅ¼yte. Tutaj rÃ³wnieÅ¼ podobieÅ„stwo wartoÅ›ci wektorÃ³w oznacza podobieÅ„stwo powiÄ…zanych z nimi informacji.

Taki embedding bÄ™dziemy wykorzystywaÄ‡ podczas pracy z bazami wektorowymi, ktÃ³re mogÄ… nam posÅ‚uÅ¼yÄ‡ do odnajdywania powiÄ…zanych ze sobÄ… danych (np. na potrzeby rekomendacji) czy wrÄ™cz przeciwnie â€” ujawniaÄ‡ odchylenia (np. na potrzeby analizy). Rola baz wektorowych polega wiÄ™c zarÃ³wno na przechowywaniu embeddingÃ³w, powiÄ…zanych z nimi metadanych umoÅ¼liwiajÄ…cych identyfikacjÄ™ zapisanych danych, jak i wykonywaniu rÃ³Å¼nych operacji, takich jak np. Similarity Search, o ktÃ³rym powiem wiÄ™cej w pÃ³Åºniejszych lekcjach.

Sentence-embedding czÄ™sto charakteryzuje takÅ¼e wiÄ™ksza liczba wymiarÃ³w, a dla modelu text-embedding-ada-002 (z ktÃ³rego bÄ™dziemy korzystaÄ‡), mÃ³wimy konkretnie o liczbie 1536. Obecnie do dyspozycji mamy takÅ¼e nowszy i wiÄ™kszy model **text-embedding-3-large** dla ktÃ³rego liczba wymiarÃ³w wynosi 3072. Poza tym do gry wchodzÄ… takÅ¼e modele Open Source, ktÃ³rych obecny ranking moÅ¼na zobaczyÄ‡ na [HuggingFace](https://huggingface.co/spaces/mteb/leaderboard). Tymczasem fragment takiego embeddingu widaÄ‡ na obrazku poniÅ¼ej.

![](https://assets.circle.so/og7a26gc1lr23jfw948iu74bkve5)

Embedding moÅ¼na takÅ¼e wygenerowaÄ‡ z pomocÄ… platformy make.com. W tym celu wystarczy skorzystaÄ‡ z akcji â€œCustom API Callâ€ do ktÃ³rej przekazujemy parametry **input** oraz **model.**

![](https://assets.circle.so/g65qfzqksdss47yz1ux0660fxnde)

JeÅ›li chcesz, moÅ¼esz zaimportowaÄ‡ powyÅ¼szy scenariusz, korzystajÄ…c ze schematu dostÄ™pnego tutaj:

*   âš¡ [Pobierz Schemat Automatyzacji](https://cloud.overment.com/aidevs_embedding-1698128103.json)
    

â—â— Do uruchomienia scenariusza konieczne jest ustawienie **struktury danych w drugim module.** Po zaimportowaniu kliknij na ten moduÅ‚, nastÄ™pnie na przycisk "Add", a pÃ³Åºniej "Generate". Tam do generatora naleÅ¼y poniÅ¼szy obiekt:

```
{
           "model": "text-embedding-3-large",                 
           "input": "{{1.prompt}}"
}
```

Na ten moment jego uÅ¼ytecznoÅ›Ä‡ jest wyÅ‚Ä…cznie edukacyjna, a sam embedding bÄ™dziemy wykorzystywaÄ‡Â w dalszych lekcjach.

**Prompt**

Z punktu widzenia osoby korzystajÄ…cej z LLM, nasza rola zaczyna siÄ™ mniej wiÄ™cej na etapie bezpoÅ›redniej interakcji z modelem. W tym celu wykorzystujemy juÅ¼ dokÅ‚adnie ten sam jÄ™zyk, ktÃ³rym posÅ‚ugujemy siÄ™ na co dzieÅ„. Jednak w zwiÄ…zku z tym, Å¼e mamy do czynienia z modelem, ktÃ³ry posiada mechanizmy "rozumienia" przekazanych mu informacji, nadal musimy pamiÄ™taÄ‡, Å¼e jest to jedynie zaawansowany model.

Pomimo tego, Å¼e np. GPT-4 Å›wietnie radzi sobie z nieustrukturyzowanym tekstem oraz potrafi realizowaÄ‡ nawet zÅ‚oÅ¼one zadania opisane prostymi sÅ‚owami, to przydatne okazujÄ… siÄ™ techniki wydawania instrukcji (tzw. promptÃ³w), ktÃ³rych zadaniem jest **wpÅ‚ywanie na zachowanie modelu.**

WspomniaÅ‚em, Å¼e dziaÅ‚anie modeli opiera siÄ™ na przewidywaniu kolejnego tokenu na podstawie **prawdopodobieÅ„stwa wystÄ…pienia kolejnego fragmentu**. CoÅ› takiego moÅ¼na bardzo wyraÅºnie zaobserwowaÄ‡ [pod tym adresem](https://platform.openai.com/playground/p/3aygR2WOaEXgz6f4l2zA9Kbq?mode=complete&model=gpt-3.5-turbo-instruct), poniewaÅ¼ Playground dla starszych modeli daje nam dostÄ™p do informacji o tym, **jakie tokeny byÅ‚y brane pod uwagÄ™ przy generowaniu odpowiedzi**.

![](https://assets.circle.so/4odwug1qe5zz7v2db34ulgwdyklo)

Tryb "Completion", ktÃ³ry widzisz na obrazku powyÅ¼ej, zawiera tylko jedno pole na prompt. **Wpisany tam tekst zostanie uzupeÅ‚niony, dÄ…Å¼Ä…c do wygenerowania sensownej odpowiedzi.** Taki mechanizm obowiÄ…zuje takÅ¼e w modelach GPT-3.5-Turbo i GPT-4, chociaÅ¼ interakcja z nimi odbywa siÄ™ w formie konwersacji. Jak niebawem siÄ™ przekonasz, prowadzenie rozmowy z modelem kaÅ¼dorazowo wymaga przesÅ‚ania jej caÅ‚ej treÅ›ci, a odpowiedÅº nadal jest uzupeÅ‚nieniem przesÅ‚anego tekstu. Podobnie teÅ¼Â o technikach interakcji z modelem, bÄ™dziemy jeszcze mÃ³wiÄ‡.

## Wnioski

Zbierzmy w caÅ‚oÅ›Ä‡ kilka zagadnieÅ„:

*   **Modele pozwalajÄ… w przybliÅ¼ony sposÃ³b** opisywaÄ‡ procesy, ktÃ³rych nie potrafimy jasno definiowaÄ‡ (np. rozpoznawanie obiektÃ³w na zdjÄ™ciach).
    
*   Sieci neuronowe mogÄ… byÄ‡ wytrenowane w celu realizacji bardzo zÅ‚oÅ¼onych zadaÅ„, **ktÃ³rych nie potrafimy opisaÄ‡ kodem**.
    
*   DuÅ¼e modele jÄ™zykowe wykorzystujÄ… sieci neuronowe do przetwarzania i generowania jÄ™zyka naturalnego.
    
*   DziaÅ‚anie modeli jÄ™zykowych wymaga (w uproszczeniu) zamiany tekstu na liczby opisujÄ…ce rÃ³Å¼ne cechy niezbÄ™dne do "zrozumienia" i generowania treÅ›ci.
    
*   Modele trenowane sÄ… na duÅ¼ych zestawach danych. Podczas treningu model ksztaÅ‚tuje zdolnoÅ›ci zwiÄ…zane z przetwarzaniem jÄ™zyka naturalnego oraz buduje swojÄ… "bazowÄ… wiedzÄ™", ktÃ³ra w przypadku modeli GPT ma rÃ³Å¼ny zakres
    
*   DuÅ¼e modele jÄ™zykowe (np. GPT-4) skupiajÄ… siÄ™ wyÅ‚Ä…cznie na generowaniu kolejnego tokenu na podstawie dotychczasowej treÅ›ci i robiÄ… to na podstawie statystyki wystÄ™powania sÅ‚Ã³w w zbiorze danych, na ktÃ³rych byÅ‚y trenowane (np. treÅ›ciach z Internetu)
    
*   W zwiÄ…zku ze zÅ‚oÅ¼onoÅ›ciÄ… LLM **nie wiemy dokÅ‚adnie, co siÄ™ dzieje pomiÄ™dzy przesÅ‚aniem wiadomoÅ›ci do modelu a generowaniem kolejnych tokenÃ³w**. Nie sÄ… jasne takÅ¼e wszystkie zachowania i umiejÄ™tnoÅ›ci modeli.
    
*   Interakcja z modelem odbywa siÄ™ poprzez naturalny jÄ™zyk i aktualnie mÃ³wimy jedynie o **sterowaniu** zachowaniem modelu, a nie peÅ‚nej kontroli, co stwarza wyzwania zwiÄ…zane z realizowaniem precyzyjnych zadaÅ„.
    
*   Generowanie kolejnych tokenÃ³w opiera siÄ™ o prawdopodobieÅ„stwo wystÄ™powania sÅ‚Ã³w w danych treningowych. **Mechanizm dobierania tokenÃ³w nie jest jednak deterministyczny**, co oznacza, Å¼e dla dokÅ‚adnie tego samego zestawu danych, moÅ¼emy otrzymywaÄ‡ rÃ³Å¼ne rezultaty.
    
*   **Architektura duÅ¼ych modeli jÄ™zykowych nakÅ‚ada na nas limit tokenÃ³w**, ktÃ³ry moÅ¼e zostaÄ‡ przetworzony w ramach pojedynczego zapytania, co wymaga podejmowania dodatkowych dziaÅ‚aÅ„ zwiÄ…zanych z np. koniecznoÅ›ciÄ… podsumowaÅ„ dÅ‚uÅ¼szych interakcji.
    

W wyniku tego:

*   Trening LLM wymaga ogromnych zestawÃ³w danych.
    
*   Ich bazowa wiedza nie obejmuje aktualnych danych.
    
*   Architektura oraz skala modeli utrudniajÄ… ich zrozumienie.
    
*   Nie posiadamy peÅ‚nej kontroli nad zachowaniem modeli.
    
*   Nie znamy wszystkich moÅ¼liwoÅ›ci modeli.
    
*   LLM potrafiÄ… realizowaÄ‡ zadania, do ktÃ³rych nie byÅ‚y trenowane.
    
*   Nie mamy peÅ‚nej kontroli nad generowaniem treÅ›ci.
    
*   Architektura modeli narzuca na nas limity.
    
*   Generowanie treÅ›ci odbywa siÄ™ na podstawie statystyki.
    
*   TreÅ›ci generowane sÄ…Â poprzez **uzupeÅ‚nianie / kontynuowanie** dotychczasowego tekstu.
    
*   Wyniki zwracane przez model sÄ…Â niedeterministyczne.
    
*   Mechanizm przewidywania kolejnego fragmentu nie sprawdza siÄ™ najlepiejÂ w przypadku zÅ‚oÅ¼onych obliczeÅ„ czy nawet przeliczania dat, ale modele mogÄ… posÅ‚ugiwaÄ‡ siÄ™ narzÄ™dziami, ktÃ³re wykonajÄ… te obliczenia
    
*   Strategia tokenizacji wpÅ‚ywa na wydajnoÅ›Ä‡ modelu w rÃ³Å¼nych jÄ™zykach. Np. modele GPT dziaÅ‚ajÄ…Â szybciej i generujÄ…Â mniejsze koszty dla jÄ™zyka angielskiego, ze wzglÄ™du na niÅ¼szÄ… liczbÄ™Â tokenÃ³w koniecznych do przetworzenia.
    
*   Uwaga modelu skupia siÄ™ jedynie na kolejnym tokenie, przez co nie posiada informacji o potencjalnych dalszych fragmentach wypowiedzi.
    
*   Z pomocÄ…Â promptÃ³w moÅ¼emy sterowaÄ‡ zachowaniem modelu.
    
*   Modele na swÃ³j sposÃ³b potrafiÄ… "rozumieÄ‡" przekazane treÅ›ci i wykorzystywaÄ‡ ten fakt podczas generowania odpowiedzi i rozwiÄ…zywania nawet zÅ‚oÅ¼onych zadaÅ„.
    
*   W zwiÄ…zku z mechanizmem "dopeÅ‚niania" wskazane jest stworzenie **przestrzeni** w postaci dÅ‚uÅ¼szego fragmentu tekstu dostarczonego przez nas lub wygenerowanego przez model, poniewaÅ¼ sprzyja to lepszemu wybieraniu kolejnych fragmentÃ³w.
    
*   Podczas pracy z modelami nasza rola w duÅ¼ym stopniu sprowadza siÄ™Â do **zwiÄ™kszenia prawdopodobieÅ„stwa uzyskania poÅ¼Ä…danej odpowiedzi**.
    

Zatrzymaj siÄ™ teraz na chwilÄ™ nad powyÅ¼szymi punktami. UwzglÄ™dniÅ‚em w nich szereg istotnych zagadnieÅ„, ktÃ³re bÄ™dÄ… pojawiaÄ‡ siÄ™ na przestrzeni wszystkich przyszÅ‚ych lekcji. JeÅ›li niektÃ³re z nich na tym etapie nie sÄ… dla Ciebie jasne, to prawdopodobnie zmieni siÄ™ to nieco pÃ³Åºniej. Daj sobie jednak przestrzeÅ„ do zapoznania siÄ™ z nimi.

## Playground

Mamy juÅ¼ wystarczajÄ…co duÅ¼o wiedzy na temat samych modeli, jednak zanim przejdziemy do kolejnych lekcji, chciaÅ‚bym zatrzymaÄ‡ siÄ™ na moment na narzÄ™dziu Playground, a konkretnie jego ustawieniach. Aktualnie praktycznie w 100% przypadkÃ³w, bÄ™dziesz korzystaÄ‡ z trybu "Chat" i to na nim skupimy swojÄ… uwagÄ™.

![](https://assets.circle.so/xlrda9p18e6qg3yajkc2sqt8mupv)

Tryb czatu charakteryzuje siÄ™ **podziaÅ‚em promptu na trzy role**, z ktÃ³rymi bÄ™dziemy siÄ™ spotykaÄ‡ takÅ¼e podczas pracy z API:

*   **System** â€” Instrukcja okreÅ›lajÄ…ca zachowanie asystenta
    
*   **User** â€” WiadomoÅ›ci uÅ¼ytkownika
    
*   **Assistant** â€” WiadomoÅ›ci AI
    

W przypadku Playground (oraz poÅ‚Ä…czenia przez API) moÅ¼esz swobodnie sterowaÄ‡ zawartoÅ›ciÄ… kaÅ¼dego pola i testowaÄ‡ zachowanie modelu w rÃ³Å¼nych sytuacjach. Jest to szczegÃ³lnie przydatne na etapie ksztaÅ‚towania promptÃ³w i ewentualnego debugowania (nie mamy typowego debuggera, ale moÅ¼emy stosunkowo Å‚atwo dostrzec, co negatywnie wpÅ‚ywa na generowanie odpowiedzi).

Po prawej stronie znajdujÄ…Â siÄ™ ustawienia:

*   **Model**: Obecnie dostÄ™pne sÄ…Â GPT-3.5-Turbo, GPT-3.5-Turbo-16k oraz GPT-4, GPT-4-turbo-preview (to ten majÄ…cy okno kontekstu do 128k tokenÃ³w), gpt-4-vision-preview (ten z moÅ¼liwoÅ›ciÄ… pracy z obrazem, ale z nim moÅ¼emy poÅ‚Ä…czyÄ‡ siÄ™ przez API), a takÅ¼e ich snapshoty, czyli wczeÅ›niejsze wersje, ktÃ³re z czasem sÄ…Â wyÅ‚Ä…czane i nie obejmujÄ… ich aktualizacje. Korzystanie z nich moÅ¼e byÄ‡Â przydatne ze wzglÄ™du na to, Å¼e wprowadzane przez OpenAI zmiany, mogÄ… negatywnie wpÅ‚ywaÄ‡Â na zachowanie Twojego systemu.
    
*   **Temperature**: To parametr wpÅ‚ywajÄ…cy na sposÃ³b wybierania tokenÃ³w, okreÅ›lany wskaÅºnikiem "kreatywnoÅ›ci" modelu. W praktyce jednak **im niÅ¼sza wartoÅ›Ä‡ temperature**, tym bardziej prawdopodobne tokeny zostanÄ…Â wybrane. Nie gwarantuje to jednak peÅ‚nej przewidywalnoÅ›ci, ale po prostu zmniejsza losowoÅ›Ä‡, co moÅ¼e byÄ‡Â przydatne przy zadaniach wymagajÄ…cych precyzji. Z kolei zwiÄ™kszenie temperatury moÅ¼e sprawdziÄ‡ siÄ™Â np. przy generowaniu kreatywnych tekstÃ³w.
    
*   **Max Length**: WartoÅ›Ä‡Â ta okreÅ›la **maksymalnÄ… liczbÄ™Â tokenÃ³w generowanych przez model**. Wiesz juÅ¼, Å¼e modele sÄ… w stanie pracowaÄ‡ jednorazowo na uzaleÅ¼nionej od wersji liczbie tokenÃ³w. **Suma tokenÃ³w w prompcie oraz "max length" nie moÅ¼e przekraczaÄ‡ dopuszczalnego limitu**. Inaczej mÃ³wiÄ…c, im dÅ‚uÅ¼szy prompt, tym mniej miejsca zostaje na generowanie odpowiedzi. Co waÅ¼ne, ustawianie **niskiej wartoÅ›ci max length, gdy spodziewasz siÄ™ krÃ³tkiej wypowiedzi, zwiÄ™ksza wydajnoÅ›Ä‡ modelu**. Z drugiej strony, ustawienie tego wskaÅºnika wysoko **nie jest zwiÄ…zane z faktycznÄ… dÅ‚ugoÅ›ciÄ…Â generowanej odpowiedzi**.
    

![](https://assets.circle.so/5vqddw6e15v06p8hebdvw3gqthh4)

*   **Stop sequences:** MoÅ¼esz tutaj przekazaÄ‡ ciÄ…gi znakÃ³w, ktÃ³re spowodujÄ… **zatrzymanie generowania odpowiedzi** (one same nie zostanÄ…Â uwzglÄ™dnione w odpowiedzi). Mowa tutaj np. o znakach nowej linii czy nawet konkretnych sÅ‚owach. Warto zachowaÄ‡ ostroÅ¼noÅ›Ä‡Â przy dobieraniu takich sekwencji, aby przypadkowo nie zatrzymaÄ‡Â odpowiedzi wczeÅ›niej, niÅ¼ tego potrzebujemy. PrzykÅ‚adem w ktÃ³rym sam korzystaÅ‚em z tej opcji, byÅ‚a praca z fine-tuningowanym modelem (fine-tuning to proces wyspecjalizowania modelu do realizacji konkretnych zadaÅ„), ktÃ³rego dane treningowe uwzglÄ™dniaÅ‚y "stop sequence" w postaci np. "-->". WÃ³wczas wiedziaÅ‚em, Å¼e pojawienie siÄ™Â takiej sekwencji zawsze powinno zakoÅ„czyÄ‡Â generowanie odpowiedzi.
    

![](https://assets.circle.so/g36z8vyge3wme7ehfiv8276ma7g6)

*   **Top P:** WskaÅºnik znany takÅ¼e jako "nucleus sampling" wpÅ‚ywajÄ…cy na dobieranie tokenÃ³w poprzez skupienie siÄ™Â na **najmniejszym zestawie tokenÃ³w, ktÃ³rych Å‚Ä…czne prawdopodobieÅ„stwo jest co najmniej rÃ³wne P (P to liczba pomiÄ™dzy 0 a 1)**. PrzykÅ‚adowo, jeÅ›li mamy zdanie "Hey, how" i kontynuacje: **"are you?" (0.5), "is it going?" (0.3),** "can I help?" (0.1) i "much is it" (0.1), a wartoÅ›Ä‡Â Top P ustawiona jest na 0.8, to pod uwagÄ™ bÄ™dÄ… wziÄ™te wyÅ‚Ä…cznie dwa pierwsze fragmenty, poniewaÅ¼Â ich Å‚Ä…czne prawdopodobieÅ„stwo wynosi 0.8. Ostatecznie wskaÅºnik Top P sprzyja zwiÄ™kszeniu kreatywnoÅ›ci wypowiedzi modelu, poniewaÅ¼ nie skupia siÄ™Â wyÅ‚Ä…cznie na najbardziej prawdopodobnych tokenach. Z drugiej strony zredukowanie jego wartoÅ›ci, zmniejszy losowoÅ›Ä‡ dobierania tokenÃ³w.
    
*   **Frequency Penalty**: To kara dla tokenÃ³w za czÄ™stotliwoÅ›Ä‡ wystÄ™powania. Gdy token pojawi siÄ™Â w odpowiedzi generowanej przez model, nastÄ™pnym razem jego prawdopodobieÅ„stwo zostanie obniÅ¼one. Ustawienie tej wartoÅ›ci zbyt wysoko, w przypadku dÅ‚uÅ¼szych wypowiedzi doprowadzi do generowania tekstu niemajÄ…cego sensu, poniewaÅ¼ tokeny, ktÃ³re normalnie powinny siÄ™Â pojawiÄ‡, bÄ™dÄ… miaÅ‚y na to zbyt maÅ‚Ä…Â szansÄ™. JednoczeÅ›nie umiarkowane zwiÄ™kszenie tego wskaÅºnika moÅ¼e byÄ‡Â pomocne w zmniejszeniu szansy na wystÄ…pienia powtÃ³rzeÅ„.
    
*   **Presence Penalty**: To kara tokenÃ³w za pojawienie siÄ™Â w wypowiedzi modelu. Jest to bardziej agresywny wskaÅºnik, poniewaÅ¼Â kara narzucana jest za samo pojawienie, a nie za ponowne wystÄ™powanie. Zastosowanie i ryzyka sÄ… podobne jak w przypadku Frequency Penalty.
    

PodsumowujÄ…c wymienione ustawienia, wiÄ™kszoÅ›Ä‡ Twojej uwagi przy pracy z modelami bÄ™dzie skupiona na parametrach **temperature** oraz **max length**. Ustawienia Top P, Frequency Penalty oraz Presence Penalty bÄ™dÄ… pomocne w przypadku zadaÅ„ wymagajÄ…cych kontrolowania rÃ³Å¼norodnoÅ›ci tekstu. Zastosowanie tych parametrÃ³w od strony praktycznej obrazuje poniÅ¼sza tabelka z wÄ…tku z [Forum OpenAI](https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683):

![](https://assets.circle.so/urgelo5kygc7xgk4jkte7wqm22cf)

## WartoÅ›ciowe ÅºrÃ³dÅ‚a wiedzy na temat LLM

Oto lista ÅºrÃ³deÅ‚ wiedzy oraz profili, ktÃ³re mogÄ™ Ci poleciÄ‡. Pozostawanie na bieÅ¼Ä…co oraz docieranie do najlepszych moÅ¼liwych ÅºrÃ³deÅ‚ wiedzy jest ogÃ³lnie istotne w kaÅ¼dej branÅ¼y. W przypadku AI, ze wzglÄ™du na przytÅ‚aczajÄ…ce tempo zmian, warto zwracaÄ‡ szczegÃ³lnÄ…Â uwagÄ™ skupienie siÄ™ na "sygnale" i pomijaniu "szumu".

*   Prompt Engineering Guide od [OpenAI](https://platform.openai.com/docs/guides/prompt-engineering), [Anthropic](https://docs.anthropic.com/claude/docs/prompt-engineering)
    
*   [OpenAI CookBook](https://cookbook.openai.com/)
    
*   [OpenAI Research](https://openai.com/research)
    
*   [Anthropic Research](https://www.anthropic.com/research)
    
*   [SpoÅ‚ecznoÅ›Ä‡ AI Explained](https://www.youtube.com/@aiexplained-official/videos)
    
*   [Generative AI od Google](https://www.cloudskillsboost.google/paths/118)
    

*   [Stephen Wolfram Writings](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
    
*   [ML Papers](https://github.com/dair-ai/ML-Papers-of-the-Week)
    
*   [AemonAlgiz](https://www.youtube.com/@AemonAlgiz)
    

*   [Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)
    
*   [Andrej Karpathy (ex OpenAI)](https://www.youtube.com/@AndrejKarpathy)
    

*   [Geoffrey Hinton](https://twitter.com/geoffreyhinton)
    
*   [Yann Lecun](https://twitter.com/ylecun)
    
*   [CS50 (Harvard)](https://www.youtube.com/@cs50)
    
*   [Newsletter Chain of Thought (CEO @ Every)](https://every.to/chain-of-thought)
    
*   [Lil'Log (OpenAI)](https://lilianweng.github.io/)
    
*   [Radek Osmulski (Nvidia)](https://radekosmulski.com/)
    
*   [Riley Goodside](https://twitter.com/goodside)
    
*   [Andrew Mayne (OpenAI)](https://andrewmayneblog.wordpress.com/)
    
*   [James Briggs (Pinecone)](https://www.youtube.com/@jamesbriggs)
    
*   [AI Explained](https://www.youtube.com/@aiexplained-official)
    
*   [All About AI](https://www.youtube.com/@AllAboutAI)
    
*   [Cognitive Revolution](https://open.spotify.com/show/6yHyok3M3BjqzR0VB5MSyk?si=93e84305d31a48bb)
    
*   [Elizabeth M. Reneiris](https://twitter.com/hackylawyER)
    
*   [Harrison Chase](https://twitter.com/hwchase17)
    
*   [Aakash Gupta](https://twitter.com/aakashg0)
    
*   [Georgi Gerganov (llama.cpp)](https://twitter.com/ggerganov)
    
*   [Fabric](https://github.com/danielmiessler/fabric/tree/main/patterns) (jakoÅ›ciowe prompty)
    
*   [Matthew Berman (modele Open Source)](https://www.youtube.com/@matthew_berman)
    

  

* * *

## Zadanie praktyczne

Zadanie na dziÅ› to "Max Tokens". Zapisz wiadomoÅ›Ä‡ systemowÄ… tak, aby zarÃ³wno jej treÅ›Ä‡, jak i wygenerowana odpowiedÅº modelu nie przekroczyÅ‚y dopuszczalnego rozmiaru kontekstu.

ğŸ”— [https://tasks.aidevs.pl/chat/maxtokens](https://tasks.aidevs.pl/chat/maxtokens)
