Wersja na [YouTube](https://youtu.be/z9S08NAUrLo).

Wiemy już, że **na ogół** duże modele językowe wykazują znacznie wyższą skuteczność (i tym samym użyteczność), gdy pracują z danymi, które im dostarczymy. Naturalnie kieruje nas to w stronę **budowania własnej bazy wiedzy, którą, dzięki integracji, model będzie w stanie się posługiwać** na różne sposoby. Zobaczmy więc, co to dokładnie oznacza.

## Koncepcja budowania / gromadzenia własnej bazy wiedzy

Domyślnie każda interakcja z modelem **rozpoczyna się od nowa**.

Wiemy już jednak, że mamy możliwość zdefiniowania **instrukcji systemowej** z pomocą której możemy dostarczyć do modelu zarówno opis oczekiwanego zachowania, jak i dodatkowe informacje na nasz temat. W ten sposób możemy sterować nie tylko zachowaniem modelu, ale także jego wiedzą.

Większość z nas nieustannie ma do czynienia z różnymi rodzajami informacji. Mogą to być linki, notatki istotne definicje, instrukcje czy rozwiązania problemów. Poza ich treścią będzie interesowało nas także zapisywanie ich źródeł, co pozwoli nam w przyszłości łatwo do nich wrócić. Oczywiście nie mówimy tutaj o "dodawaniu strony do zakładek" i zapomnieniu o niej na zawsze, ponieważ teraz, w korzystaniu ze zgromadzonej wiedzy, może wspierać nas LLM.

Przykładem może być łatwe sięganie do dokumentacji narzędzi z których korzystamy (np. palety kolorów Tailwind CSS). Świetnie sprawdza się także zapisywanie rozwiązań problemów które sprawiły nam kłopot, szczególnie jeśli istnieje ryzyko, że spotkamy je za kilka miesięcy ponownie.

Dostęp do **spersonalizowanych informacji** pozwala także modelowi na **ograniczoną autonomię**, ponieważ może nie potrzebować naszej aktywności w celu dostarczenia danych potrzebnych do wykonania aktualnego zadania. Tutaj przykładem może być klasyfikacja nowego wpisu, do której konieczne jest pobranie opisów pasujących kategorii. Zamiast wczytywać je wszystkie do promptu systemowego, wybieramy tylko te, które zostały wskazane na etapie wcześniejszej klasyfikacji (będziemy o tym jeszcze mówić). Oczywiście nadal takie działania **powinny być przez Ciebie nadzorowane** na wypadek błędów lub nieoczekiwanego zachowania.

Poza pracą z tekstem, obecne AI pozwala nam także na stosunkowo proste **przetwarzanie różnych formatów danych, np. audio czy obrazów**. To z kolei otwiera przed nami możliwości budowania mechanizmów pozwalających na skanowanie oraz analizę wybranych źródeł wiedzy (np. listy stron www czy wybranych API). Jednym z przykładów zastosowań może być **prywatny newsletter czy nawet podcast**, generowane na podstawie tak zebranych danych.

Niewykluczone, że posiadasz już jakieś doświadczenia związane z budowaniem własnej bazy wiedzy czy po prostu struktury bazy danych na potrzeby projektów rozwijanych w pracy. Jeśli tak, to wiesz już, że nie jest to proste zadanie, chociażby ze względu na **organizowanie treści** oraz jej **przeszukiwanie**. Jednak łącząc bazę wiedzy z modelem, możesz **zautomatyzować lub znacznie zoptymalizować** te procesy.

**Baza danych**

Baza wiedzy dla LLM może być organizowana w praktycznie dowolnym miejscu do którego będziemy mieć swobodny dostęp. Zwykle będzie to np. mySQL, PostgreSQL czy MongoDB. W przypadku narzędzi dobrym wyborem może być Airtable lub nawet Notion. Całkiem interesująco wypada także [xata.io](https://xata.io/) (trafiłem na ten projekt kilka dni temu z mocnego polecenia. Nie miałem jednak okazji realizować w nim większych projektów). W tym miejscu dodam jeszcze, że w niektórych przypadkach interesować nas będą także bazy grafowe, choć w ich przypadku moje doświadczenie w tej chwili nie jest zbyt duże, natomiast mają sporo mojej uwagi.

Niezależnie od wyboru bazy danych, konieczne jest zaplanowanie tego, **jakie informacje chcemy w niej przechowywać** oraz to, **w jaki sposób będziemy je organizować.** Poniżej znajduje się przykład mojej bazy wiedzy połączonej z LLM, która może stanowić pewien punkt odniesienia.

W swojej bazie wiedzy wyróżniam:

*   **Wspomnienia**: czyli wszystko to, co jest związane bezpośrednio ze mną oraz moim najbliższym otoczeniem
    
*   **Notatki**: czyli moje szybkie notatki na różne tematy. Zwykle są to cytaty, instrukcje czy fragmenty artykułów
    
*   **Kreacje:** czyli wygenerowane grafiki / obrazy
    
*   **Zasoby:** czyli głównie linki do różnych stron, artykułów
    
*   **Akcje:** czyli opisy umiejętności i dane niezbędne do połączenia z zewnętrznymi usługami
    
*   **Wiadomości:** czyli cała historia interakcji pomiędzy mną a AI
    

![](https://assets.circle.so/3or01qdum86qznx1x7fbq6bj4tiz)

(^ obrazek pochodzi z IntelliJ)

W wersji minimalnej bazą wiedzy dla Twojego systemu może być nawet **kilka prostych plików tekstowych** z których będzie pobierał podstawowe informacje, które pomogą mu wykonać wybrane zadania. Przykładowo w jednym z nich mogą być opisane Twoje obecne umiejętności związane z konkretną technologią, co może być wykorzystane (tak jak pokazywałem już w poprzednich lekcjach) na potrzeby skanowania / podsumowania artykułu w sposób dopasowany do Ciebie. W kontekście biznesowym może być to wiedza na temat kategorii produktu, która pozwoli lepiej wygenerować / moderować jego opis.

**Gromadzenie danych**

Treści mogą pojawiać się w bazie **automatycznie, półautomatycznie lub być dodawane ręcznie.** Dlatego przy wyborze bazy danych, warto zwrócić uwagę na **możliwość połączenia przez API**.

Prawdopodobnie najprostszym rozwiązaniem, jest skorzystanie z Airtable oraz [Make.com](http://make.com/), ale ono sprawdzi się raczej do prywatnych zastosowań lub potrzeb małego biznesu. Bardziej zaawansowanym (ale także bardziej elastycznym) rozwiązaniem jest **zbudowanie własnego API**, podłączenie domeny i skonfigurowanie serwera.

Konkretnie mówimy tutaj o **zbudowaniu scenariusza automatyzacji**, którego wyzwalaczem jest Webhook. W chwili, gdy zostaną przesłane do niego dane, scenariusz uruchomi się i zapisze je w naszej bazie danych. Poniżej znajduje się **przykładowy i dość zaawansowany scenariusz**, który wykorzystuję do **zapisania linków na później**.

![](https://assets.circle.so/xg9vfe5b3ptw9l36l4zewkj9jhjg)

Taki scenariusz może być uruchomiony automatycznie przez **inny scenariusz** lub **makro** działające na moim komputerze (Shortcuts/Autohotkey) lub iPhone. Możliwe jest także dodawanie takich linków poprzez integrację ze Slackiem, Telegramem lub Discordem, które można dość łatwo utworzyć w [](http://make.com/)make.com

![](https://assets.circle.so/wirbemsa7ru3d4lpy7jgnz2jwcbw)

Prostą integrację omówimy na przykładzie Slacka (na bardziej złożoną przyjdzie jeszcze czas), jednak możesz ją odwzorować z pomocą aplikacji Shortcuts, Autohotkey, szablonu aplikacji Tauri, którą udostępniałęm w jednej z pierwszych lekcji, czy własnych skryptów. Pamiętaj, że **możesz przejść dowolną ścieżką i skorzystać z narzędzi na które się zdecydujesz.**

Zatem pierwszym krokiem będzie utworzenie struktury bazy. Możesz tutaj skorzystać z przygotowanego przeze mnie, prostego szablonu. Aby to zrobić, po przejściu na link poniżej, wybierz przycisk (⚡ use this data) w lewym, górnym rogu ekranu.

[Skopiuj szablon](https://airtable.com/appwMSq5CdeokCeMR/shrp60f73wIni3zAB)

> Uwaga: w tej bazie znajduje się na tym etapie kilka dodatkowych pól i dodatkowa tabela. W tej chwili możesz je zignorować 🙂

![](https://assets.circle.so/d8nyvtb3u0hbfmsybvfkmi4jpj16)

Następnie możesz skorzystać albo z Airtable SDK i podłączyć się do tej bazy z pomocą kodu i własnej aplikacji, albo skorzystać z platformy [make.com](http://make.com/), co właśnie zrobimy. Konkretnie interesuje nas podłączenie się do kanału Slack (w moim przypadku będzie to publiczny kanał "Links") w celu nasłuchiwania nowych wiadomości.

Gdy niebawem aktywujemy scenariusz, nowo wysłane wiadomości będą go uruchamiały, co pozwoli nam na przesłanie ich treści do OpenAI w celu opisania, a następnie do naszej bazy Airtable.

![](https://assets.circle.so/5cgy0yre4z3k5ibw6uso8zin84nt)

Jak widać, scenariusz jest prosty, ale potrzebuje konfiguracji. W tym celu:

1.  Utwórz nowy / pusty scenariusz make.com
    
2.  W menu na dole wybierz przycisk "..." (trzech kropek) i zaimportuj schemat: [Pobierz Blueprint](https://cloud.overment.com/aidevs_memory_basic-1711559341.json)
    
3.  Utwórz konto Slack lub od razu kanał na koncie, które już posiadasz (musi być prywatne, ponieważ firmowe zwykle posiadają ograniczone uprawnienia. Poza tym chcemy aby to była nasza prywatna integracja)
    
4.  Wróć do Make i kolejno podłącz: Konto na Slacku oraz wskaż odpowiedni kanał, konto OpenAI, konto Airtable ([tutaj](https://airtable.com/create/tokens) możesz wygenerować token dostępu)
    
5.  Po podłączeniu kont, wyślij testową wiadomość na Slacku, np. z prośbą o zapisanie adresu URL, a następnie uruchom scenariusz.
    

Dodam tylko, że jeśli nie chcesz korzystać ze Slacka ani innego komunikatora, możesz skorzystać z natywnego modułu "Webhook" dostępnego w [make.com](http://make.com/).

W tym miejscu na szczególną uwagę zasługuje prompt odpowiadający za ustrukturyzowanie wiadomości przesłanej na Slacku, na poszczególne kolumny z bazy danych. Jego treść wygląda następująco:

````
Extract key information from the user's message and represent it as a JSON object using this format:

```json
{
  "content": "Memory description based solely on the user's input",
  "type": "note|memory|resource",
  "source": "comma-separated URLs, if provided"
}
```

Follow these guidelines:
- `content`: Concisely summarize the essential information from the user's message
- `type`: 
  - Use `"note"` only if the user explicitly states they are providing a note
  - Use `"resource"` only if the user mentions a link to be remembered  
  - Use `"memory"` if the user asks you to remember a fact that doesn't fit the other categories
- `source`: Include relevant URLs, separated by commas, if the user provides any

Remember:
- Focus exclusively on the user's input; do not add any extra information
- Keep the `content` summary brief but informative
- Ensure the output is a valid JSON object with properly formatted and escaped values
- If no URLs are provided, set `source` to an empty string
- Ask for clarification if the input is ambiguous or incomplete

Example:
User: Please remember that my favorite color is blue.
Assistant: ```json
{
  "content": "User's favorite color is blue",
  "type": "memory",
  "source": ""
}
````

Jak widać, posiada on instrukcję, przykład oczekiwanej struktury obiektu, opisy poszczególnych właściwości oraz listę zasad, których ma się trzymać. Dodatkowo podałem w tym przypadku jeden przykład, jednak zwykle będzie nam zależało na podaniu ich przynajmniej kilku.

Dodatkowo interesujący jest tutaj fakt, że do opracowania powyższego promptu wykorzystałem inny prompt i model Claude 3 Opus. W ramach mojej wiadomosći opisałem tylko wymagania jakie mam wobec promptu, który ma zostać wygenerowany, a resztą zajął się LLM.

Zatem nasz “metaprompt” wygląda następująco (dodam tylko, że umieszczałem go już na kanale "Dyskusje", więc niewykluczone, że go już znasz):

```
As a prompt engineer, I'll do my best to craft, optimize and enchance prompts for GPT-4. My knowledge is based on the context below, that is a Prompt Engineering Guide from OpenAI.
What we need here is to talk about the prompt and pick the strategy (long or short prompt) and then shape its structure using available knowledge from the guide. We may iterate this prompt throughout the conversation. When asked, do your best to tell why LLM may not behave as expected and use your intuition to suggest fixes.

Always skip headers and make prompt look natural. Always prompt with triple backticks.

Guide###
Prompt Structure (except for simple task) has to be splitted into multiple paragraphs and bullet lists, like this one below (skip square brackets [] and headers). Always write prompts in Markdown.
[Role Description & Overall goal]
[Instruction that describes how to reach the goal]
[List of rules]
[List of examples that shows clearly what user may say and how AI will respond. Examples has to show the overall pattern and expected behaviour in the edge cases]
[General context (like current date)]
[Context / External knowledge. Use separators here to make a clear distinction]

Make sure that you won't include final response of the model, because our job here is to write a prompt and nothing else. 

Overall Strategies: 
- Write clear instructions: These models can’t read minds. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you’d like to see. The less the model has to guess at what you want, the more likely you’ll get it. Useful tactics: Include details in your query to get more relevant answers, Ask the model to adopt a persona, Use delimiters (like triple hashtags or backticks) to clearly indicate distinct parts of the input, Provide examples, Specify the desired length of the output.

- Provide reference text: Language models can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to these models can help in answering with fewer fabrications. Useful tactics: Instruct the model to answer using a reference text, Instruct the model to answer with citations from a reference text

- Give the model time to "think": If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, models make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a "chain of thought" before an answer can help the model reason its way toward correct answers more reliably. Useful tactics: Instruct the model to work out its own solution before rushing to a conclusion, Use inner monologue or a sequence of queries to hide the model's reasoning process, Ask the model if it missed anything on previous passes

- Use external tools: Use JSON generation that will serve as a payload for the connection with an external API. Give the model access to specific functions and descriptions of the avaialable endpoints and parameters. 

# Useful phrases to use within prompt you'll create steer model's behaviour: 

Limiting Output
“Do not output warnings or notes—just the requested sections.”
“Do not include any extra commentary or explanations.”
“Only output the requested information, without any additional context.”

Preventing Repetition
“Do not repeat ideas, quotes, facts, or resources.”
“Ensure each point is unique and not a rephrasing of a previous point.”
“Avoid redundancy in your output.”

Encouraging Diversity
“Do not start items with the same opening words.”
“Vary your sentence structure and vocabulary throughout the output.”
“Use diverse examples and phrasings to illustrate your points.”

Focusing on the Task
“DO NOT COMPLAIN. Just create the output as instructed.”
“If the task seems complex, find a way to simplify it and provide the best output possible.”
“Focus solely on the given instructions and provide the most relevant output.”

Following Instructions
“Do not deviate from or question the given instructions.”
“Ensure you follow ALL the provided instructions when creating your output.”
“Adhere strictly to the outlined steps and requirements.”

Preventing Assumptions
“Do not make assumptions about the input or rely on information that may be incorrect.”
“Base your output only on the facts and context provided, without extrapolating.”
“If the input is unclear or contradictory, seek clarification rather than making assumptions.”

Encouraging Thoroughness
“Ensure your output is comprehensive and fully addresses the given prompt.”
“Provide a complete and detailed response that covers all relevant aspects.”
“Take the time to thoroughly analyze the input before providing your output.”

Specifying Output Format
“Output your response using valid, human-readable Markdown syntax.”
“Format your output as specified, using the provided examples as a guide.”
“Ensure your output follows the designated structure and style guidelines.”
###
```

Powyższy prompt również opracowałem przy współpracy z LLM, jednak w tej sytuacji opierałem się o swoją wiedzę na temat projektowania instrukcji dla modelu oraz zewnętrzne źródła w postaci poradnika OpenAI czy [repozytorium Fabric](https://github.com/danielmiessler/fabric).

Choć w podanym wyżej przykładzie wykorzystałem ten "metaprompt" w bezpośredniej rozmowie z modelem, to nic nie stoi na przeszkodzie, aby wykorzystać jego wariant do stworzenia systemu działającego w formie "agenta", którego zadanie będzie polegało nie tylko na wygenerowaniu nowego promptu, ale także jego przetestowanie. Przykładem takiego projektu może być [GPT-Prompt-Engineer](https://github.com/mshumer/gpt-prompt-engineer/blob/main/opus_to_haiku_conversion.ipynb).

## Praca z formatami audio / wideo

Tworzenie notatek głosowych jest bardzo wygodne, ale wymaga dodatkowej pracy związanej z ich faktycznym przepisywaniem. Większość rozwiązań zdolnych do zamiany audio na tekst działa ze skutecznością na poziomie 90%+. Niestety w tych 10% uwzględniamy **słowa kluczowe i zwroty**, które nierzadko kształtują główny przekaz. Inaczej wygląda to w przypadku modelu Whisper, którego transkrypcje wydają się być perfekcyjne (a przynajmniej do tej pory jego skuteczność na podstawie moich notatek oceniam na 99.5%).

Samo utworzenie notatki głosowej jest stosunkowo proste, bo obecnie już chyba każdy telefon posiada taką funkcjonalność. W przypadku systemu iOS możesz także skorzystać z Siri Shortcuts. Niezależnie od wybranego sposobu, chodzi o **uzyskanie pliku audio**. Tutaj zaznaczam także, że nagranie może pochodzić również z materiału wideo (skrypt wykorzystujący ffmpeg może Ci w tym pomóc).

Następnie potrzebujesz zadbać o to, aby **nagrania audio trafiły automatycznie do skryptu odpowiedzialnego za generowanie transkrypcji z pomocą modelu Whisper**. Można to osiągnąć na różne sposoby. Jednym z nich jest wspomniane makro Shortcuts, które może przesłać nagranie audio do scenariusza [make.com](http://make.com/), ale dokładnie to samo można osiągnąć także z pomocą wiadomości głosowej na Slacku, natomiast taki scenariusz zbudujemy sobie w dalszych lekcjach.

![](https://assets.circle.so/qcjb5cac8ir5hg5czjcgutxmtjz9)

Scenariusz make może po prostu tworzyć transkrypcję **i zapisywać ją w naszej bazie Airtable** (lub dowolnym innym miejscu z którym może skontaktować się przez API). Zanim jednak to się wydarzy, notatka może zostać odpowiednio sformatowana oraz podzielona na sekcje (np. podsumowanie, główne punkty, akcje). Warto jednak zadbać o to, aby **poza zmodyfikowaną treścią, zapisać także oryginał, aby można było się do niego łatwo odwołać**.

![](https://assets.circle.so/96hdsn6gvw3ria9eykgkr6e907ct)

*   ⚡ [Pobierz makro Shortcut](https://www.icloud.com/shortcuts/90028338bbfc4c7a991db87f1e78ad56)
    
*   ⚡ [Pobierz blueprint Make.com](https://cloud.overment.com/aidevs_voice-1695287808.json)
    

[voice\_memo.mp4](https://assets.circle.so/qao09c4hp82jq1tlm9zqj8wqd825)

Jeśli pracujesz w innym systemie niż macOS lub po prostu nie chcesz korzystać z Shortcuts, to możesz skorzystać z Dropbox / Google Drive, aby zautomatyzować proces transkrypcji i formatowania. Wówczas wystarczy jeden dodatkowy scenariusz, który będzie **obserwował wybrany przez Ciebie katalog**, a następnie **przesyłał nowo dodane pliki do scenariusza, który już mamy**. W ten sposób unikniemy duplikowania logiki, co jest także dobrym przykładem pewnego stylu myślenia, który możesz stosować podczas projektowania mechaniki asystenta (i to nie tylko w kontekście no-code, ale przede wszystkim programowania).

Mianowicie, scenariusz, który mamy poniżej faktycznie **obserwuje wybrany katalog na Google Drive**, następnie **pobiera nowo dodany plik** i **przesyła go z pomocą modułu HTTP** na webhook naszego **wcześniejszego scenariusza**.

![](https://assets.circle.so/dntsssf23m5ahoaus1v157jau8ew)

*   ⚡ [Pobierz Blueprint Scenariusza](https://cloud.overment.com/process-1695303035.json)
    

Konfiguracja powyższej automatyzacji polega więc jedynie na:

1.  Zaimportowaniu blueprintu
    
2.  Podłączenie konta Google do modułów Google Drive
    
3.  Podmienienie adresu URL w ostatnim module, na adres **webhooka wygenerowanego we wcześniejszym scenariuszu**
    

To wszystko! Od teraz niezależnie od tego w jakiej formie nagrasz notatkę audio, wystarczy, że prześlesz ją do obserwowanego folderu.

Zanim przejdziemy dalej, chciałbym zwrócić uwagę na kilka rzeczy, które pozwolą Ci wykorzystać powyższą koncepcję oraz wiele innych, nie tylko w sposób, który Ci prezentuję. Otóż możesz pomyśleć o notatkach głosowych jako treści na podstawie której:

*   generowany jest obiekt JSON (podobnie jak w przypadku szybkich notatek Alice), który **bezpośrednio zostaje przesłany do Twojej aplikacji do zadań, kalendarza czy systemu CRM**
    
*   pobierane są wybrane treści w ustalonym formacie. Np. lista zakupów, treść wpisu do budżetu domowego czy nawet szkic wiadomości e-mail
    
*   formatowanie może być bardzo zaawansowane i uwzględniać nawet kroki, które pozwolą na jej podstawie generować wpisy do mediów społecznościowych
    
*   dodane notatki głosowe w kolejnych lekcjach będą mogły trafić do **pamięci długoterminowej Twojego asystenta**
    
*   notatki głosowe mogą być wzbogacane dodatkowymi opisami (np. linkami), które trudno jest podyktować. Wystarczy, że dodasz kolejny krok, który pozwoli Ci podać takie dane
    
*   przetwarzanie notatek głosowych może odbywać się w połączeniu **z dynamicznym kontekstem** (nawet bardzo prostym, uwzględniającym opisy Twoich projektów), korzystając z technik omawianych w dotychczasowych lekcjach. Wiele z tych wątków będziemy także rozszerzać niebawem
    
*   z notatki głosowej mogą być także pobrane **całe listy akcji**, które zrealizują dla Ciebie automatyzacje, skrypty czy po prostu Twój przyszły asystent AI. Warto jednak tutaj albo dopracować prompt, albo (idealnie) weryfikować wygenerowaną listę przed jej wykonaniem
    
*   interakcje głosowe mogą być także połączone z Twoimi komunikatorami, w celu wysyłania lub nawet odbierania wiadomości głosowych. Tutaj jednak pamiętaj o tym, że treści trafiają na serwery OpenAI, więc zweryfikuj wcześniej politykę prywatności. Sam korzystam z takiej funkcjonalności na potrzeby rozmowy z Alice i pokażę ją bliżej w ostatnim tygodniu kursu (zobacz obrazek poniżej)
    

![](https://assets.circle.so/d7q4atg35q7okgkhulni4zh5wz2h)

  

## Praca z otwartymi formatami (np. Markdown)

Myślę, że na tym etapie doskonale rozumiesz już to, co podkreślałem w kontekście **łatwego dostępu do danych** oraz pracy na możliwie otwartych formatach. Pracując z AI nieco bliżej, zauważysz, że nierzadko uzasadnione będzie nawet **zmienienie stacku aplikacji**, niż próba stworzenia interakcji z narzędziami, które nie dają łatwego dostępu do treści z poziomu kodu lub automatyzacji.

Z tego powodu, zawsze staram się dobierać aplikacje oraz usługi, z którymi pracuję, kierując się **jakością API, które oferują**. Co więcej, nie zawsze obecność API jest wystarczająca. Na przykład, ActiveCampaign (platforma do e-mail marketingu) udostępnia pokaźną listę endpointów, ale nie pozwala w pełni zautomatyzować zarządzania kampaniami z poziomu API.

Wybierając narzędzia z których można korzystać **bez graficznego interfejsu,** odgrywają istotną rolę z punktu widzenia automatyzacji. Jednak zupełnie nowego wymiaru nabierają w kontekście AI, ponieważ **LLM mogą się nimi posługiwać**.

Mówiąc jednak konkretnie, warto:

*   budować **prywatne API** w postaci aplikacji lub zestawu scenariuszy automatyzacji, które będzie zdolne do realizowania różnych zadań i **łączenia ze sobą wielu usług oraz źródeł danych**
    
*   budować **własną bazę wiedzy**, idealnie opartą o PostgreSQL (lub inne narzędzie dopasowane do naszych preferencji) albo narzędzia no-code, np. Airtable. Kluczowy jest tutaj **łatwe zarządzanie treścią z poziomu API**, ponieważ wówczas będziemy mogli zaangażować do tego AI
    
*   wybierać narzędzia do notatek / zadań / kalendarza / poczty, które pozwalają na łatwy dostęp do treści. Sam na potrzeby tworzenia treści wykorzystuję iA Writer (bezpłatna alternatywa to [Focused](https://www.71squared.com/focused)) oraz Obsidian. Do zadań wykorzystuję Notion a pocztę i kalendarz mam w usługach Google.
    

Naturalnie, jak zwykle, decyzja należy tutaj do Ciebie. Jednak na podstawie własnych doświadczeń, opisane podejście **znacznie ułatwia pracę z dużymi modelami językowymi** oraz zintegrowanie ich ze swoją codziennością (lub jej elementami). Przez podobne zagadnienia **możesz** przechodzić także w sytuacji, gdy będziesz pracować nad komercyjnymi wdrożeniami

* * *

## Zadanie praktyczne

1.  Korzystając z modelu Whisper wykonaj zadanie API (zgodnie z opisem na [tasks.aidevs.pl](https://tasks.aidevs.pl/)[)](http://zadania.aidevs.pl/) o nazwie **whisper**. W ramach zadania otrzymasz plik MP3 (15 sekund), który musisz wysłać do transkrypcji, a otrzymany z niej tekst odeślij jako rozwiązanie zadania.